{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines and Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The Linear Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Iris data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = skl.datasets.load_iris()\n",
    "print(\"Number of cases: {}\\t Number of features: {}\\n\".format(iris_dataset['data'].shape[0],iris_dataset['data'].shape[1]))\n",
    "print(\"Class names: {}\".format(iris_dataset['target_names']))\n",
    "print(\"Feature names: {}\".format(iris_dataset['feature_names']))\n",
    "f1 = 1\n",
    "f2 = 2\n",
    "plt.scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],c=iris_dataset['target'])\n",
    "plt.xlabel(iris_dataset['feature_names'][f1])\n",
    "plt.ylabel(iris_dataset['feature_names'][f2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing to two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_class = (iris_dataset['target'] == 0).astype(int)\n",
    "versicolor_class = (iris_dataset['target'] == 1).astype(int)\n",
    "virginica_class = (iris_dataset['target'] == 2).astype(int)\n",
    "\n",
    "print(\"Original class labels: \\n{}\\n\".format(iris_dataset['target']))\n",
    "print(\"setosa_class:  \\n{}\\n\".format(setosa_class))\n",
    "\n",
    "\n",
    "plt.scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],c=setosa_class)\n",
    "plt.xlabel(iris_dataset['feature_names'][f1])\n",
    "plt.ylabel(iris_dataset['feature_names'][f2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a linear (pure) SVM (using only 2 out of the 4 Iris features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = SVC(kernel='linear').fit(iris_dataset['data'][:,[f1,f2]],setosa_class)\n",
    "plt.scatter(iris_dataset['data'][:,1],iris_dataset['data'][:,f2],c=setosa_class)\n",
    "mglearn.plots.plot_2d_separator(linear_svm,iris_dataset['data'][:,[f1,f2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying and visualizing the support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support vectors: \\n{}\\n\".format(linear_svm.support_vectors_))\n",
    "plt.scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],c=setosa_class)\n",
    "plt.scatter(linear_svm.support_vectors_[:,0],linear_svm.support_vectors_[:,1],c='red')\n",
    "mglearn.plots.plot_2d_separator(linear_svm,iris_dataset['data'][:,[f1,f2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the scalability of SVC on linearly separable data. We are using 'blobs' data that looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=np.array([[2,2],[-2,-2]]), n_samples=1000)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=18.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n_samples gets large, the two generated blobs will almost surely overlap, i.e., the data is\n",
    "then not linarly separable. A linear decision boundary will still be appropriate for the data, and\n",
    "SVC can handle this non linearly separable data.\n",
    "\n",
    "Now we create blobs data of varying size, and measure the time it takes to learn a linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=np.array([100,1000, 10000,100000])\n",
    "#sizes=np.array([100,500,1000,50000,100000,500000])\n",
    "times=np.zeros(sizes.size)\n",
    "for i,s in enumerate(sizes):\n",
    "    X, y = make_blobs(centers=np.array([[2,2],[-2,-2]]), n_samples=s)\n",
    "    start=time.time()\n",
    "    linear_svm = SVC(kernel='linear').fit(X,y)\n",
    "    end=time.time()\n",
    "    times[i]=end-start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.yscale('log')\n",
    "plt.xlabel('size')\n",
    "\n",
    "plt.plot(sizes,times,'ro')\n",
    "plt.plot(sizes,times,'b-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try a larger range of datasizes (e.g. start with the commented 'sizes' array above). You may need to plot using log-scale on the x-axis. What do you observe? What happens when you change the coordinates of the blob centers (initially set to [2,2] and[-2,-2])? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The nonlinear case:  kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to classify Iris Versicolor with a linear SVM as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 1\n",
    "f2 = 2\n",
    "linear_svm = SVC(kernel='linear').fit(iris_dataset['data'][:,[f1,f2]],versicolor_class)\n",
    "plt.scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],c=versicolor_class)\n",
    "plt.xlabel(iris_dataset['feature_names'][f1])\n",
    "plt.ylabel(iris_dataset['feature_names'][f2])\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm,iris_dataset['data'][:,[f1,f2]])\n",
    "\n",
    "plt.show()\n",
    "print(\"Accuracy (on train): {}\".format(linear_svm.score(iris_dataset['data'][:,[f1,f2]],versicolor_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can do better when we include all 4 features of Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = SVC(kernel='linear').fit(iris_dataset['data'],versicolor_class)\n",
    "print(\"Accuracy (on train): {}\".format(linear_svm.score(iris_dataset['data'],versicolor_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not help much! Let's go back to only 2 features, and try some standard nonlinear kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel_svm = SVC(kernel='rbf', gamma=1.0).fit(iris_dataset['data'][:,[f1,f2]],versicolor_class)\n",
    "plt.scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],c=versicolor_class)\n",
    "plt.scatter(kernel_svm.support_vectors_[:,0],kernel_svm.support_vectors_[:,1],c='red')\n",
    "mglearn.plots.plot_2d_separator(kernel_svm,iris_dataset['data'][:,[f1,f2]])\n",
    "print(\"Accuracy (on train): {}\".format(kernel_svm.score(iris_dataset['data'][:,[f1,f2]],versicolor_class)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the gamma parameter of the rbf kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammavals = np.array([0.1,1.0,10.0,50.0])\n",
    "accuracies = np.zeros(gammavals.size)\n",
    "fig,axes = plt.subplots(gammavals.size,figsize=(10,20))\n",
    "for i,g in enumerate(gammavals):\n",
    "    currentax = axes.ravel()[i]\n",
    "    kernel_svm = SVC(kernel='rbf', gamma=g).fit(iris_dataset['data'][:,[f1,f2]],versicolor_class)\n",
    "    accuracies[i]=kernel_svm.score(iris_dataset['data'][:,[f1,f2]],versicolor_class)\n",
    "    currentax.set_title(\"Gamma: {}  Train accuracy: {}\".format(g,accuracies[i]))\n",
    "    mglearn.plots.plot_2d_separator(kernel_svm, iris_dataset['data'][:,[f1,f2]], ax=currentax)\n",
    "    mglearn.discrete_scatter(iris_dataset['data'][:,f1],iris_dataset['data'][:,f2],versicolor_class,ax=currentax)\n",
    "    mglearn.discrete_scatter(kernel_svm.support_vectors_[:,0],kernel_svm.support_vectors_[:,1],c='red', ax=currentax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Now do a train/test split of the Iris data, and check for different Gamma values the train and the test \n",
    "accuracy. At which Gamma do you observe overfitting? The following cell already contains the code for getting train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'][:,[f1,f2]],versicolor_class, stratify=versicolor_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Using Custom Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a positive-semi definite Kernel matrix (we compute and print the eigenvalues to verify whether our matrix is positive definite, i.e., only has positive eigenvalues):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "kmatrix = np.array([[0.5,0.4,0.1,0.6,0.5],[0,.5,0.6,0.4,0.4],[0,0,0.5,0.3,0.1],[0,0,0,0.5,0.2],[0,0,0,0,0.5]])\n",
    "kmatrix = kmatrix + kmatrix.transpose()\n",
    "print(\"Matrix:\\n{}\".format(kmatrix))\n",
    "print(\"Eigenvalues:\\n{}\".format(eig(kmatrix)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1,-1,-1,-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn an SVM classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kernel_svm = SVC(kernel='precomputed').fit(kmatrix,labels)\n",
    "my_kernel_svm.predict(kmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the trained SVM to a new test instance, given by its kernel values with respect to the training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_kernel_vals=np.array([[0.5,0.5,0.1,0.4,0.5]])\n",
    "my_kernel_svm.predict(test_kernel_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
