{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this notebook is partly based on pp.106-120 of \"Introduction to Machine Learning with Python\". Some code snippets are directly taken from that source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The Linear Version: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons,make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mglearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,f1_score,\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report)\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a very simple, linearly separable dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=10)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a perceptron, and plotting the decision regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[],activation='identity',solver='lbfgs').fit(X, y)\n",
    "print('Number of outputs: {}, output activations: {}'.format(mlp.n_outputs_,mlp.out_activation_))\n",
    "mglearn.plots.plot_2d_separator(mlp, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learns a linear decision boundary that separates the two classes. Observe that the 'activation' parameter only sets the activation function for hidden neurons, not the output.\n",
    "\n",
    "We next add an \"outlier\" to the blue class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.vstack([X,[6.5,-1.0]])\n",
    "X = np.vstack([X,[6,0.0]])\n",
    "y = np.append(y,0.0)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =34\n",
    "mlp = MLPClassifier(hidden_layer_sizes=[],activation='identity',solver='lbfgs', random_state=seed).fit(X, y)\n",
    "print(\"Loss: {}\".format(mlp.loss_))\n",
    "mglearn.plots.plot_2d_separator(mlp, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is shifted, so that the blue outlier still gets correctly classified. But **note**: the Sklearn implementation of MLPClassifier does not minimize the traditional Perceptron loss shown on Slide 11. It minimizes the *log-loss*, which in this case still leads to minimizing the misclassification error.\n",
    "\n",
    "Compare with SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "linear_svm = SVC(kernel='linear').fit(X,y)\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB().fit(X,y)\n",
    "mglearn.plots.plot_2d_separator(nb, X, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes does not avoid mis-classifications \"at all cost\". Also correctly classified examples count: the higher the likelihood, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The nonlinear case: adding hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first NN with one hidden layer with 10 nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[10],activation='relu',solver='lbfgs', random_state=0, max_iter=500).fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is piecewise linear. The different line segments correspond to different subsets of hidden units that output '0'. The output unit then computes a linear function of the outputs from the other hidden units, which is a linear function of the input. This is also still the case with several hidden 'relu' layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[5,5],activation='relu',solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 'tanh' activation function, the boundary becomes smooth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[10,10],activation='tanh',solver='lbfgs').fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware: with this small datasize, the result is very unstable with respect to the random initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randstates = np.arange(10)\n",
    "accuracies = np.zeros(10)\n",
    "losses=np.zeros(10)\n",
    "fig,axes = plt.subplots(2,5,figsize=(20,8))\n",
    "for i in randstates:\n",
    "    currentax = axes.ravel()[i]\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=[5,5],activation='relu',solver='lbfgs', random_state=i*10).fit(X_train, y_train)\n",
    "    accuracies[i]=mlp.score(X_test,y_test)\n",
    "    losses[i]=mlp.loss_\n",
    "    currentax.set_title(\"Test accuracy: {}\\n {}\".format(accuracies[i],losses[i]))\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3,ax=currentax)\n",
    "    mglearn.discrete_scatter(X_test[:,0],X_test[:,1],y_test,ax=currentax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** investigate how the variability and accuracy of the results depends on the structure of the NN as defined by hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Solvers and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying regularization: starting with a relatively small sample (N=500), and lbfgs solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, stratify=Y, random_state=42)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([0.0,0.001,0.1,0.3,0.5,0.7,1.0,5.0])\n",
    "train_accs = np.zeros(alphas.size)\n",
    "test_accs = np.zeros(alphas.size)\n",
    "\n",
    "fig,axes = plt.subplots(alphas.size,2,figsize=(10,50))\n",
    "\n",
    "for i,a in enumerate(alphas):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=[10,10],activation='tanh',solver='lbfgs', max_iter=2000,\\\n",
    "                        alpha=a,random_state=5).fit(X_train,Y_train)\n",
    "    train_pred=mlp.predict(X_train)\n",
    "    test_pred=mlp.predict(X_test)\n",
    "    train_accs[i]=accuracy_score(train_pred,Y_train)\n",
    "    test_accs[i]=accuracy_score(test_pred,Y_test)\n",
    "    \n",
    "    currentax = axes[i,0]\n",
    "    currentax.set_title(\"alpha: {}, training data\".format(a))\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3,ax=currentax)\n",
    "    mglearn.discrete_scatter(X_train[:,0],X_train[:,1],Y_train,ax=currentax)\n",
    "    \n",
    "    currentax = axes[i,1]\n",
    "    currentax.set_title(\"alpha: {}, test data\".format(a))\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3,ax=currentax)\n",
    "    mglearn.discrete_scatter(X_test[:,0],X_test[:,1],Y_test,ax=currentax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')    \n",
    "plt.plot(alphas,train_accs,label='train')\n",
    "plt.plot(alphas,test_accs,label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Investigate the effect of the alpha regularization parameter in connection with other data/model/learning parameters: \n",
    "<ul>\n",
    "    <li>the noise level in the make_moons data generator</li>\n",
    "    <li>the neural network structure</li>\n",
    "    <li>the activation function</li>\n",
    "    <li>the choice of the solver (try Adam instead of lbfgs)</li>\n",
    "</ul>\n",
    "When does the alpha parameter make a big difference in what is learned, and when is it beneficial in improving the accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
