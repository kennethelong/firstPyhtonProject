{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy as scipy\n",
    "import seaborn as sns\n",
    "\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "X_full = iris_data['data']\n",
    "y_full = iris_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the feature names and target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features names: {}\".format(iris_data['feature_names']))\n",
    "print(\"Target names: {}\".format(iris_data['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "\n",
    "In particular, we wish to investigate the data relative to the naive Bayes distributional assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we divide the data conditional on the class values (here we just considwer the entire data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations for setosa\n",
    "X_0 = X_full[y_full==0,:]\n",
    "# Observations for versicolor\n",
    "X_1 = X_full[y_full==1,:]\n",
    "# Observations for virginica\n",
    "X_2 = X_full[y_full==2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a closer look at the distribution of the data. \n",
    "\n",
    "First for the setosa class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X_0,columns=iris_data['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then for `versicolor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X_1,columns=iris_data['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally for `virginica`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X_2,columns=iris_data['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined as a Gaussian Naive Bayes. This means that the four features are assumed to follow Gaussian distributions conditional on the class:\n",
    "\n",
    "$$ f(x_i|y) = \\frac{1}{\\sqrt{2\\cdot \\pi\\cdot \\sigma_y^2}}\\exp \\bigg (\\frac{-(x_i-\\mu_y)^2}{2\\cdot \\sigma_y^2}\\bigg )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mu = 0\n",
    "_sigma = 1\n",
    "_x = np.linspace(_mu-5*_sigma,_mu+5*_sigma, 1000)\n",
    "plt.plot(_x, scipy.stats.norm.pdf(_x, _mu, _sigma))\n",
    "plt.ylim(0,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the model is fitted to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'])\n",
    "nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predicitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single instance, we can make a classification and get class distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = X_test[0,:]\n",
    "print(\"Instance to be classified: {}\".format(inst))\n",
    "print(\"Predicted class: {}\".format(iris_data['target_names'][nb.predict([inst])[0]]))\n",
    "print(\"Predicted class probabilities: {}\".format(nb.predict_proba([inst])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do we perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy score: {}\".format(nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the model\n",
    "\n",
    "We can open up the model and investigate some of the learned distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The learned prior distribution for the class: {}\".format(nb.class_prior_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, this matches the empirical distribution in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class distribution: {}\".format(counts/sum(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The conditional variance learned by the model for the setosa class: {}\".format(nb.var_[0,:]))\n",
    "print(\"The conditional means learned by the model for the setosa class: {}\".format(nb.theta_[0,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the empirical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_0[:,0])\n",
    "sns.distplot(X_0[:,1])\n",
    "sns.distplot(X_0[:,2])\n",
    "sns.distplot(X_0[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling with binary features\n",
    "\n",
    "For illustration we shall consider a *modified* version of the `house votes` data:\n",
    "\n",
    "* This data set includes votes for each of the U.S. House of       Representatives Congressmen on the 16 key votes identified by the       CQA.  The CQA lists nine different types of votes: voted for, paired       for, and announced for (these three simplified to yea), voted       against, paired against, and announced against (these three       simplified to nay), voted present, voted present to avoid conflict       of interest, and did not vote or otherwise make a position known       (these three simplified to an unknown disposition).\n",
    "\n",
    "* Attributes:\n",
    "   1. Class Name: 2 (democrat, republican)\n",
    "   2. handicapped-infants: 2 (y,n)\n",
    "   3. water-project-cost-sharing: 2 (y,n)\n",
    "   4. adoption-of-the-budget-resolution: 2 (y,n)\n",
    "   5. physician-fee-freeze: 2 (y,n)\n",
    "   6. el-salvador-aid: 2 (y,n)\n",
    "   7. religious-groups-in-schools: 2 (y,n)\n",
    "   8. anti-satellite-test-ban: 2 (y,n)\n",
    "   9. aid-to-nicaraguan-contras: 2 (y,n)\n",
    "   10. mx-missile: 2 (y,n)\n",
    "   11. immigration: 2 (y,n)\n",
    "   12. synfuels-corporation-cutback: 2 (y,n)\n",
    "   13. education-spending: 2 (y,n)\n",
    "   14. superfund-right-to-sue: 2 (y,n)\n",
    "   15. crime: 2 (y,n)\n",
    "   16. duty-free-exports: 2 (y,n)\n",
    "   17. export-administration-act-south-africa: 2 (y,n)\n",
    " \n",
    "* In this modified data set, missing vote values have been replaced by the most frequently occuring vote of a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and replace 'y' and 'n' with 1 and o ´, respectively.\n",
    "df = pd.read_csv(\"votes-complete.csv\", sep=\",\")\n",
    "df.replace('y',1, inplace=True)\n",
    "df.replace('n',0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:].values\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and split the data into training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model\n",
    "\n",
    "Since we deal with binary features will will uses the model `BernoulliNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nbb = BernoulliNB(alpha=1.00)\n",
    "#nbb = BernoulliNB(alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: {}\".format(nbb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the model\n",
    "\n",
    "We can open up the model and investigate some of the learned distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The learned distribution for the class, P(Class): {}\".format(np.exp(nbb.class_log_prior_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The learned distributions for the features given Class=rep.: \")\n",
    "pd.DataFrame([np.exp(nbb.feature_log_prob_[1][:])], columns=df.columns[1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The learned distributions for the features given Class=dem.: \")\n",
    "pd.DataFrame([np.exp(nbb.feature_log_prob_[0][:])], columns=df.columns[1:].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multinomial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial NB is suitable for modeling discrete features representing count data such as word counts for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the model we will look at the twenty newsgroup dataset:\n",
    "\n",
    "*The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.*\n",
    "\n",
    "To reduce complexity we consider only 4 news groups: `categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(twenty_train.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target label\n",
    "print(twenty_train['target_names'][twenty_train['target'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of files: {}\".format(len(twenty_train.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data representation\n",
    "\n",
    "We will use a bag of words representation, where a document is represented by the frequency with which the different words appear in the text. (Data is stored as a sparse matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text and (possibly) remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the word 'to'\n",
    "count_vect.vocabulary_.get(u'to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of times 'to' appears in document '0'\n",
    "X_train_counts[0,32493]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer documents will have higher count values than shorter documents even if they concern the same topic. We avoid this problem by dividing the number of occurences of a word by the number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast', 'Data science is fun']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tf = tf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
