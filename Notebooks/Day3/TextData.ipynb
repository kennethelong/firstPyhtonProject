{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,f1_score,\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a dataset containing movie reviews from the *Internet Movie Database*. For this, the data first needs to be downloaded from <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\"> here </a>. Note: this is about 220 Mb.  After uncompressing, the data is contained in a directory `aclImdb` with sub-directories `train` and `test`. In the following, replace the piece of the path that leads to the directory in which you have unpacked the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\KennethElong(KEEL)\\Downloads\\aclImdb_v1\\aclImdb\\train'\n",
    "\n",
    "# Load the training data\n",
    "reviews_train = load_files(data_path, categories=['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "b'\"Show People\" is an absolutely delightful silent directed by King Vidor and starring Marion Davies and Billy Haines. What gems both of them are in this charming comedy about a young girl, Peggy Pepper, whose acting is the talk of Savannah trying to make it on the big screen. Though she\\'s a success in comedy, what she wants to do is make \"art\" so she moves up to High Arts Studio. Soon she becomes Patricia Pepoire and is too good for the likes of her friend Billy.<br /><br />Many stars of the silent era have cameos in \"Show People,\" including Davies herself without the curly hair and makeup. I\\'m sure when people saw the film in 1928, they recognized everyone who appeared in the elaborate lunch scene; sadly, nowadays, it\\'s not the case, even for film buffs. In one part of the film, however, she does meet Charlie Chaplin; in another, author Elinor Glyn is pointed out to her, and Vidor himself has a cameo at the end of the film. Other stars who pop up in \"Show People\" are John Gilbert, Douglas Fairbanks, William S. Hart, Leatrice Joy, Bess Flowers, Renee Adoree, Rod LaRoque, Aileen Pringle, and many others.<br /><br />Davies was adorable and a lively comedienne. It\\'s a shame William Haines quit the movies - he was cute and energetic, deservedly an enormous star back in the day.<br /><br />\"Show People\" is a simple story told in a witty way. It\\'s also a look back at an exciting era in Hollywood\\'s history and contains performances by two wonderful stars.'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train.target_names)\n",
    "revidx = 15\n",
    "print(reviews_train.data[revidx])\n",
    "print(reviews_train.target[revidx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dictionary that maps tokens appearing in the data to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=CountVectorizer(min_df=0.0005, max_df=0.5).fit(reviews_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary contains 15862 entries\n",
      "The index of the word 'horrible' in the dictionary is 6953\n",
      "The word at index 708 is 'an'\n"
     ]
    }
   ],
   "source": [
    "print(\"The dictionary contains {} entries\".format(len(dictionary.vocabulary_)))\n",
    "wrd='horrible'\n",
    "#wrd='is'\n",
    "\n",
    "print(\"The index of the word '{}' in the dictionary is {}\".format(wrd,dictionary.vocabulary_[wrd]))\n",
    "widx=708\n",
    "print(\"The word at index {} is '{}'\".format(widx, dictionary.get_feature_names_out()[widx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary construction with `CountVectorizer` does not include stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horrendous' 'horrendously' 'horrible' 'horribly' 'horrid']\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.get_feature_names_out()[6951:6956])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the data into feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of 'reviews_train_vec' is <class 'scipy.sparse._csr.csr_matrix'> with 25000 rows and 15862 columns\n"
     ]
    }
   ],
   "source": [
    "reviews_train_vec=dictionary.transform(reviews_train.data)\n",
    "\n",
    "print(\"The type of 'reviews_train_vec' is {} with {} rows and {} columns\".format(type(reviews_train_vec),reviews_train_vec.shape[0],reviews_train_vec.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix structure becomes visible, when we print the first 1000 entries of the row for the review at index `revidx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 11 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 44)\t1\n",
      "  (0, 262)\t1\n",
      "  (0, 271)\t1\n",
      "  (0, 354)\t1\n",
      "  (0, 430)\t1\n",
      "  (0, 640)\t1\n",
      "  (0, 708)\t3\n",
      "  (0, 788)\t1\n",
      "  (0, 857)\t1\n",
      "  (0, 972)\t1\n",
      "  (0, 989)\t1\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train_vec[revidx,0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing 0s are still 'there':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(reviews_train_vec[revidx,400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also construct a tf-idf representation of the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 11 stored elements and shape (1, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 44)\t0.11583415233723711\n",
      "  (0, 262)\t0.02736708009501029\n",
      "  (0, 271)\t0.05703878107352525\n",
      "  (0, 354)\t0.037194533735317274\n",
      "  (0, 430)\t0.09538182464530867\n",
      "  (0, 640)\t0.034471169026461794\n",
      "  (0, 708)\t0.07543383127629115\n",
      "  (0, 788)\t0.04295469192449846\n",
      "  (0, 857)\t0.07717698456902591\n",
      "  (0, 972)\t0.06196111008037628\n",
      "  (0, 989)\t0.0834549762184734\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_train=TfidfTransformer().fit(reviews_train_vec)\n",
    "reviews_train_tfidf_vec=tfidf_transformer_train.transform(reviews_train_vec)\n",
    "print(reviews_train_tfidf_vec[revidx,0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the feature vector for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb=MultinomialNB().fit(reviews_train_vec,reviews_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and transforming the test data. The dictionary used for the test data is the one constructed from the training data! Also the transformation with the idf values is done using the TfidfTransformer constructed from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test=load_files(r'C:\\Users\\KennethElong(KEEL)\\Downloads\\aclImdb_v1\\aclImdb\\train',categories=['neg','pos'])\n",
    "reviews_test_vec=dictionary.transform(reviews_test.data)\n",
    "reviews_test_tfidf_vec = tfidf_transformer_train.transform(reviews_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the learned model to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: \n",
      " 0.87324\n",
      "\n",
      "Accuracy on train: \n",
      " 0.87324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train=mnb.predict(reviews_train_vec)\n",
    "predictions_test=mnb.predict(reviews_test_vec)\n",
    "\n",
    "print(\"Accuracy on test: \\n {}\\n\".format(accuracy_score(reviews_test.target,predictions_test)))\n",
    "print(\"Accuracy on train: \\n {}\\n\".format(accuracy_score(reviews_train.target,predictions_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out about what words are most indicative of postive/negative reviews, by looking at the parameters of the model.\n",
    "\n",
    "The attribute `feature_log_prob_` returns the log probabilities of the different features (words) under the two classes. By taking the difference for the two classes, we get a measure for how much a word discriminates between the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_diff=mnb.feature_log_prob_[0,:]-mnb.feature_log_prob_[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `np.argsort` to obtain the indices of the values in log_prob_diff in increasing order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4636   814  4116  6443  6210 15802 10285 15226  1626  6127  1851  3723\n",
      "  6576 10163  9993 14676 12154  1200  8898  3610  1609  7923 14593  6806\n",
      " 12647  8048  9862 10302  6439  5534]\n"
     ]
    }
   ],
   "source": [
    "sorted_idxs=np.argsort(log_prob_diff)\n",
    "print(sorted_idxs[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and retrieve the words corresponding to these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 words most discriminating for positive reviews are:\n",
      "\n",
      "edie\n",
      "antwone\n",
      "din\n",
      "gunga\n",
      "goldsworthy\n",
      "yokai\n",
      "paulie\n",
      "visconti\n",
      "blandings\n",
      "gino\n",
      "brashear\n",
      "deathtrap\n",
      "harilal\n",
      "panahi\n",
      "ossessione\n",
      "tsui\n",
      "sabu\n",
      "aweigh\n",
      "mcintire\n",
      "daisies\n",
      "\n",
      "\n",
      "The 20 words most discriminating for negative reviews are:\n",
      "\n",
      "weisz\n",
      "wayans\n",
      "dyer\n",
      "dahmer\n",
      "rosanna\n",
      "dunaway\n",
      "savini\n",
      "beowulf\n",
      "seagal\n",
      "thunderbirds\n",
      "manos\n",
      "shaq\n",
      "btk\n",
      "saif\n",
      "kareena\n",
      "hobgoblins\n",
      "tashan\n",
      "slater\n",
      "uwe\n",
      "boll\n"
     ]
    }
   ],
   "source": [
    "numfeats=20\n",
    "print(\"The {} words most discriminating for positive reviews are:\\n\".format(numfeats))\n",
    "for i in sorted_idxs[0:numfeats]:\n",
    "    print(dictionary.get_feature_names_out()[i])\n",
    "print(\"\\n\")    \n",
    "print(\"The {} words most discriminating for negative reviews are:\\n\".format(numfeats))\n",
    "for i in sorted_idxs[len(sorted_idxs)-numfeats:len(sorted_idxs)]:\n",
    "    print(dictionary.get_feature_names_out()[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that we are overfitting due to very rare words. We can inspect instead words that are a little bit away from the extreme ends of the sorted log_prob_diff vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with 'positive rank' between 500 and 520:\n",
      "\n",
      "rainer\n",
      "liu\n",
      "einstein\n",
      "sergeants\n",
      "conductor\n",
      "amicus\n",
      "companionship\n",
      "seamlessly\n",
      "demme\n",
      "trading\n",
      "corinne\n",
      "swordplay\n",
      "cheryl\n",
      "moriarty\n",
      "brilliantly\n",
      "iran\n",
      "shanghai\n",
      "mesmerizing\n",
      "favorites\n",
      "sullivan\n",
      "\n",
      "\n",
      "Words with 'positive rank' between 15342 and 15362:\n",
      "\n",
      "aimlessly\n",
      "forehead\n",
      "humourless\n",
      "countdown\n",
      "inexcusable\n",
      "drawl\n",
      "walston\n",
      "horribly\n",
      "horrendous\n",
      "rambo\n",
      "lackluster\n",
      "hulk\n",
      "dafoe\n",
      "rehash\n",
      "bother\n",
      "simpson\n",
      "rubber\n",
      "cringing\n",
      "costs\n",
      "dumb\n"
     ]
    }
   ],
   "source": [
    "numfeats=20\n",
    "offset = 500\n",
    "print(\"Words with 'positive rank' between {} and {}:\\n\".format(offset,offset+numfeats))\n",
    "for i in sorted_idxs[offset:offset+numfeats]:\n",
    "    print(dictionary.get_feature_names_out()[i])\n",
    "print(\"\\n\")    \n",
    "print(\"Words with 'positive rank' between {} and {}:\\n\".format(len(sorted_idxs)-numfeats-offset,len(sorted_idxs)-offset))\n",
    "for i in sorted_idxs[len(sorted_idxs)-numfeats-offset:len(sorted_idxs)-offset]:\n",
    "    print(dictionary.get_feature_names_out()[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try a neural network classifier on the tf-idf transformed data next. We use a fairly strong regularization with `alpha=0.5` to compensate for the strong overfitting opportunities in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(alpha=0.5).fit(reviews_train_tfidf_vec,reviews_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is runnig, we can take a little break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: \n",
      " 0.92484\n",
      "\n",
      "Accuracy on train: \n",
      " 0.92484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train=mlp.predict(reviews_train_tfidf_vec)\n",
    "predictions_test=mlp.predict(reviews_test_tfidf_vec)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test: \\n {}\\n\".format(accuracy_score(reviews_test.target,predictions_test)))\n",
    "print(\"Accuracy on train: \\n {}\\n\".format(accuracy_score(reviews_train.target,predictions_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the reviews that are evaluated as most positive (negative) by the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3770  7586 19599 ...  2667 23628 14849]\n"
     ]
    }
   ],
   "source": [
    "most_positive=np.argsort( mlp.predict_proba(reviews_test_tfidf_vec)[:,0] )\n",
    "print(most_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3 reviews ranked as most positive are:\n",
      "\n",
      "b\"If you have not seen this excellent movie about life in the 90s (in L.A.) then you've missed a special treat. This is one of the most amazingly and most powerful movies ever made about life for Americans in the 90s and it even carries over into today's world in which we live in. It covers everything from raising a child, prejudice (more than one way),love, adultery, empty nest syndrome, selfishness, etc..and the list goes on. This story builds up to an ultimate climax and then when nothing else matters it always goes back to love with friends and family and love of life. It helps us dig deep within ourselves and to make us search for what we want out of life. Makes us ask questions of ourselves. Have we done enough for others, are we like this, etc.??? Sit back and enjoy a wonderfully done and emotional movie that I'm sure others will enjoy for a lifetime.<br /><br />Take note of Mary Mcdonnell, Kevin Kline and Danny Glover's wonderful performance through this whole film. These actors are amazing and really show the true glow and meaning of what message is being sent to all of us. These are 3 of my favorite actors for life after seeing this film over 10 years ago now. I still enjoy it again and again. Also enjoy the wonderful soundtrack with it and don't forget to count how many times you see the helicopter fly by and try to figure out it's symbolism for the movie??hmmm... I almost forgot this is probably Steve Martin's very first serious acting role in any film he has ever done. He, too does an excellent job in this movie. This may come as a surprise to most of you. Sit back, relax and enjoy truly good film making.....\"\n",
      "\n",
      "\n",
      "\n",
      "b\"This is one of the most underrated masterpieces of all time in my opinion, its thought provoking, funny and sad with amazing performances all around!. All the characters are wonderful, and the story is just brilliant!, plus Jodie Foster and Cherie Currie are simply amazing in this!. The Ending is very powerful, however I won't spoil it for you, and I thought the character development was top notch!, plus you can really relate to all of the characters, especially Jeanie and Annie, as you will be rooting for them!, plus I loved how it moved slowly, and giving you a chance to get to know all the characters and what there about. I can't believe this only has a 5.9 rating on here as it should be much higher in my opinion, and it was funny seeing Randy Quaid in this type of role, plus this is extremely well written and made as well!. One scene that really got to me was when Madge(Marilyn Kagan), is totally embarrassed by her mother for having the party, and the film has many surprising moments as well!, plus the dialog is especially excellent. This is one of the most underrated masterpieces of all time (In my opinion), its thought provoking, funny and sad with amazing performances all around, and i say Go see it immediately!, your bound to love it!. The Direction is fantastic!. Adrian Lyne does a fantastic job here, with awesome camera work, and keeping the film at an extremely engrossing pace!. The Acting is amazing!. Jodie Foster is really cute, and is amazing as always!, she was extremely likable, caring, had a lovable character, was intense in some scenes, was focused, and she and Cherie Currie were the heart of the film as Jeanie and Annie!(Foster Rules!!!!!!!). Cherie Currie is way hot, and is amazing here, i really felt sorry for her character, as she had a very likable character that just needed help, she gives a powerful performance, and created a very memorable character she was amazing!. Scott Baio is great as Brad he was really likable, and did his job well i liked him. Randy Quaid is great in his serious role surprisingly i liked him. Sally Kellerman is great as the mother i liked her a lot. Marilyn Kagan and Kandice Stroh are both very good as Madge and Deirdre, and did what they had to do well as the other two friends. Laura Dern has a very early role here, as it was cool to see her, not much of a part though. Rest of the cast do fine. Overall go see it immediately, it's an underrated masterpiece!. ***** out of 5\"\n",
      "\n",
      "\n",
      "\n",
      "b\"This is one of the most beautiful films I have ever seen. The Footage is extraordinary, mesmerizing at times. It also received an Oscar for best photography, and deservedly so. I have many movies in my film collection and several more I've seen besides them, and not many of them are more beautifully or even equally as beautifully shot as this one.<br /><br />It's unique and an overall great movie. The cast is terrific and do a great job in portraying their characters. We follow their destinies with devotion, and get very emotionally attached to them. Along the way, we also learn things about ourselves and our lives. I think much of this film for what it represent, and how it present it. I warmly recommend it\"\n",
      "\n",
      "\n",
      "\n",
      "The 3 reviews ranked as most negative are:\n",
      "\n",
      "b'This movie was so bad! It was terrible! It was awful! I cannot stress it enough! The acting, directing, story, characters and everything about it was bad! It was so corny and clich\\xc3\\xa9d. Don\\'t be fooled by the cover, or the tag line \"The \\'texas massacre\\' is nothing to laugh at.\" Are you frogging\\' kidding me! It was ridiculous.<br /><br />The first 2 minutes of the film is good until it gets to the main character Brendan, OK now turn it off. What I got from the film was, A bunch of ugly, annoying and immature people go to a cabin in the middle of the woods and a clown that sings nursery rhymes kills them in unoriginal and fake ways.<br /><br />This movie was a waste of my time and money, and it would be a waste of your money and time too! I fast forward through most of the movie because it was so terrible, I just wanted to see how each bad actor died, and it STILL wasn\\'t worth it! Just looking at the cover is a waste of time. This IS seriously THE worst movie EVER! Rating: doesn\\'t deserve one.'\n",
      "\n",
      "\n",
      "\n",
      "b'Terrible acting, lame plot, stupid story and just all around terrible movie sums up this piece of junk. It was excruciating to sit through. Just awful. Do not waste one penny on this. The movie theaters should feel bad about actually putting this movie out there for people to watch. This \"horror\" film was not even in the least bit scary, creepy or disturbing. It was in no way visually appealing. The acting was so terrible by all of the actors that any attempt to draw you into the movie through dialog are completely destroyed within moments of the actor/actress opening their mouth. Plus the entire story, i don\\'t know why someone would make a movie with this story AGAIN. Do not waste your time or money. Even if it\\'s a free ticket don\\'t waste one moment viewing this movie. You will feel dumber for watching it.'\n",
      "\n",
      "\n",
      "\n",
      "b\"I would have given it a one instead of a two, but I suppose it COULD have been worse. I guess the acting isn't all that bad, but the plot lacks anything even remotely close to interesting. It is a terrible movie!! TERRIBLE! Complete waste of time! I strongly suggest you do not watch this movie.!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numrevs = 3\n",
    "\n",
    "print(\"The {} reviews ranked as most positive are:\\n\".format(numrevs))\n",
    "for i in most_positive[0:numrevs]:\n",
    "    print(reviews_test.data[i])\n",
    "    print(\"\\n\\n\")\n",
    " \n",
    "print(\"The {} reviews ranked as most negative are:\\n\".format(numrevs))\n",
    "for i in most_positive[len(most_positive)-numrevs:len(most_positive)]:\n",
    "    print(reviews_test.data[i])\n",
    "    print(\"\\n\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LDA requires some rather time-consuming computations, we first create a smaller dictionary than we had before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary contains 1665 entries\n"
     ]
    }
   ],
   "source": [
    "dictionary=CountVectorizer(min_df=0.01, max_df=0.15).fit(reviews_train.data)\n",
    "print(\"The dictionary contains {} entries\".format(len(dictionary.vocabulary_)))\n",
    "reviews_train_vec=dictionary.transform(reviews_train.data)\n",
    "reviews_train_tfidf_vec=TfidfTransformer().fit_transform(reviews_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model with `ntopics` latent topics. The LDA learner has a parameter `max_iter` that determines how many iterations over the whole dataset are performed. A *perplexity* score measures how well the learned model explains/fits the data. When time permits, one can see how the perplexity score improves when one allows more iterations in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36.290708780288696s\n",
      "Iterations performed: 5\n",
      "Perplexity score: 1389.1235697432812\n",
      "Time: 65.17953634262085s\n",
      "Iterations performed: 10\n",
      "Perplexity score: 1392.8813228559225\n"
     ]
    }
   ],
   "source": [
    "ntopics=20\n",
    "\n",
    "maxiters = np.array([5,10])\n",
    "\n",
    "for m in maxiters:\n",
    "    start=time.time()\n",
    "    lda = LatentDirichletAllocation(n_components=ntopics,learning_method='online',max_iter=m).fit(reviews_train_vec)\n",
    "    end=time.time()\n",
    "    print(\"Time: {}s\".format(end-start))\n",
    "    print(\"Iterations performed: {}\".format(lda.n_iter_))\n",
    "    print(\"Perplexity score: {}\".format(lda.perplexity(reviews_train_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `components_` contains the word probabilities for the latent topics (not strictly probabilities, because they do not sum to 1 over all words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1665)\n",
      "The first 30 words have the following 'probabilities':\n",
      " [7.10779694e+02 5.00000011e-02 1.29698903e+02 2.84837450e+02\n",
      " 5.00000015e-02 1.79242398e+02 5.00000020e-02 5.00000018e-02\n",
      " 5.00000007e-02 5.00000009e-02 5.00000034e-02 5.00000017e-02\n",
      " 5.00000011e-02 5.00000014e-02 5.00000015e-02 8.65089105e+01\n",
      " 5.00000011e-02 2.94644677e+02 5.00000006e-02 5.00000009e-02\n",
      " 5.00000010e-02 5.00000012e-02 5.00000009e-02 4.58518071e+01\n",
      " 6.48061723e+01 5.46702109e+01 5.00405434e-02 1.46751627e+02\n",
      " 5.00000025e-02 5.00000009e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "f=30\n",
    "print(\"The first {} words have the following 'probabilities':\\n {}\\n\".format(f,lda.components_[0,0:f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate which words have the highest probabilities under the different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "\n",
      "years\n",
      "wonderful\n",
      "mr\n",
      "excellent\n",
      "am\n",
      "saw\n",
      "again\n",
      "always\n",
      "loved\n",
      "amazing\n",
      "michael\n",
      "thought\n",
      "job\n",
      "parents\n",
      "beautiful\n",
      "seeing\n",
      "enjoyed\n",
      "favorite\n",
      "every\n",
      "10\n",
      "\n",
      "\n",
      "Topic 1\n",
      "\n",
      "action\n",
      "horror\n",
      "effects\n",
      "genre\n",
      "special\n",
      "lee\n",
      "fans\n",
      "dr\n",
      "sequences\n",
      "de\n",
      "atmosphere\n",
      "battle\n",
      "gore\n",
      "fight\n",
      "evil\n",
      "dark\n",
      "may\n",
      "however\n",
      "style\n",
      "elements\n",
      "\n",
      "\n",
      "Topic 2\n",
      "\n",
      "world\n",
      "children\n",
      "earth\n",
      "animation\n",
      "japanese\n",
      "voice\n",
      "our\n",
      "beautiful\n",
      "fantasy\n",
      "spirit\n",
      "animated\n",
      "child\n",
      "eyes\n",
      "animals\n",
      "power\n",
      "giant\n",
      "beauty\n",
      "dog\n",
      "live\n",
      "images\n",
      "\n",
      "\n",
      "Topic 3\n",
      "\n",
      "us\n",
      "american\n",
      "human\n",
      "true\n",
      "without\n",
      "understand\n",
      "real\n",
      "perhaps\n",
      "nature\n",
      "stories\n",
      "british\n",
      "history\n",
      "hollywood\n",
      "able\n",
      "emotional\n",
      "important\n",
      "based\n",
      "self\n",
      "viewer\n",
      "whether\n",
      "\n",
      "\n",
      "Topic 4\n",
      "\n",
      "funny\n",
      "comedy\n",
      "fun\n",
      "humor\n",
      "laugh\n",
      "jokes\n",
      "10\n",
      "hilarious\n",
      "cast\n",
      "comic\n",
      "moments\n",
      "entertaining\n",
      "perfect\n",
      "filled\n",
      "laughs\n",
      "christmas\n",
      "actors\n",
      "highly\n",
      "enjoy\n",
      "comedies\n",
      "\n",
      "\n",
      "Topic 5\n",
      "\n",
      "years\n",
      "music\n",
      "dvd\n",
      "novel\n",
      "song\n",
      "now\n",
      "dance\n",
      "remember\n",
      "later\n",
      "version\n",
      "saw\n",
      "star\n",
      "last\n",
      "released\n",
      "release\n",
      "few\n",
      "since\n",
      "title\n",
      "god\n",
      "night\n",
      "\n",
      "\n",
      "Topic 6\n",
      "\n",
      "killer\n",
      "police\n",
      "murder\n",
      "wife\n",
      "gets\n",
      "david\n",
      "death\n",
      "blood\n",
      "killed\n",
      "night\n",
      "cop\n",
      "crime\n",
      "80\n",
      "mystery\n",
      "ending\n",
      "prison\n",
      "slasher\n",
      "detective\n",
      "killing\n",
      "plays\n",
      "\n",
      "\n",
      "Topic 7\n",
      "\n",
      "black\n",
      "white\n",
      "car\n",
      "red\n",
      "rock\n",
      "daughter\n",
      "joe\n",
      "sequence\n",
      "down\n",
      "100\n",
      "night\n",
      "run\n",
      "door\n",
      "later\n",
      "trip\n",
      "dream\n",
      "south\n",
      "new\n",
      "party\n",
      "gets\n",
      "\n",
      "\n",
      "Topic 8\n",
      "\n",
      "book\n",
      "jack\n",
      "new\n",
      "tom\n",
      "york\n",
      "romantic\n",
      "city\n",
      "books\n",
      "steve\n",
      "indian\n",
      "successful\n",
      "comedic\n",
      "romance\n",
      "london\n",
      "marriage\n",
      "nor\n",
      "results\n",
      "neither\n",
      "based\n",
      "hotel\n",
      "\n",
      "\n",
      "Topic 9\n",
      "\n",
      "game\n",
      "play\n",
      "actors\n",
      "difficult\n",
      "james\n",
      "match\n",
      "theater\n",
      "team\n",
      "brilliant\n",
      "impossible\n",
      "find\n",
      "touching\n",
      "year\n",
      "happening\n",
      "gorgeous\n",
      "boys\n",
      "heart\n",
      "directing\n",
      "news\n",
      "superb\n",
      "\n",
      "\n",
      "Topic 10\n",
      "\n",
      "show\n",
      "series\n",
      "tv\n",
      "episode\n",
      "kids\n",
      "episodes\n",
      "shows\n",
      "television\n",
      "disney\n",
      "season\n",
      "musical\n",
      "fi\n",
      "sci\n",
      "new\n",
      "songs\n",
      "numbers\n",
      "singing\n",
      "star\n",
      "now\n",
      "original\n",
      "\n",
      "\n",
      "Topic 11\n",
      "\n",
      "girl\n",
      "town\n",
      "thriller\n",
      "local\n",
      "tries\n",
      "find\n",
      "small\n",
      "takes\n",
      "goes\n",
      "meets\n",
      "finds\n",
      "comes\n",
      "starts\n",
      "take\n",
      "doesn\n",
      "decides\n",
      "kills\n",
      "later\n",
      "island\n",
      "becomes\n",
      "\n",
      "\n",
      "Topic 12\n",
      "\n",
      "worst\n",
      "awful\n",
      "video\n",
      "10\n",
      "terrible\n",
      "waste\n",
      "horrible\n",
      "am\n",
      "actors\n",
      "stupid\n",
      "effects\n",
      "worse\n",
      "thing\n",
      "kids\n",
      "crap\n",
      "special\n",
      "want\n",
      "looked\n",
      "check\n",
      "script\n",
      "\n",
      "\n",
      "Topic 13\n",
      "\n",
      "director\n",
      "script\n",
      "production\n",
      "work\n",
      "actors\n",
      "camera\n",
      "interesting\n",
      "music\n",
      "budget\n",
      "looks\n",
      "low\n",
      "look\n",
      "seems\n",
      "nothing\n",
      "rather\n",
      "though\n",
      "making\n",
      "dialogue\n",
      "sex\n",
      "direction\n",
      "\n",
      "\n",
      "Topic 14\n",
      "\n",
      "men\n",
      "between\n",
      "women\n",
      "own\n",
      "rather\n",
      "each\n",
      "point\n",
      "relationship\n",
      "world\n",
      "different\n",
      "both\n",
      "seems\n",
      "work\n",
      "woman\n",
      "lives\n",
      "us\n",
      "our\n",
      "place\n",
      "events\n",
      "real\n",
      "\n",
      "\n",
      "Topic 15\n",
      "\n",
      "original\n",
      "version\n",
      "part\n",
      "king\n",
      "ending\n",
      "sequel\n",
      "remake\n",
      "stories\n",
      "hardly\n",
      "younger\n",
      "bit\n",
      "van\n",
      "quite\n",
      "looks\n",
      "pretty\n",
      "attractive\n",
      "parts\n",
      "doesn\n",
      "costumes\n",
      "between\n",
      "\n",
      "\n",
      "Topic 16\n",
      "\n",
      "war\n",
      "scary\n",
      "world\n",
      "french\n",
      "our\n",
      "monster\n",
      "ben\n",
      "science\n",
      "documentary\n",
      "us\n",
      "brain\n",
      "camp\n",
      "military\n",
      "fiction\n",
      "army\n",
      "drive\n",
      "soldiers\n",
      "hero\n",
      "green\n",
      "hey\n",
      "\n",
      "\n",
      "Topic 17\n",
      "\n",
      "performance\n",
      "role\n",
      "john\n",
      "cast\n",
      "actor\n",
      "wife\n",
      "plays\n",
      "oscar\n",
      "played\n",
      "performances\n",
      "director\n",
      "supporting\n",
      "roles\n",
      "both\n",
      "scott\n",
      "play\n",
      "actress\n",
      "american\n",
      "tony\n",
      "mark\n",
      "\n",
      "\n",
      "Topic 18\n",
      "\n",
      "family\n",
      "father\n",
      "young\n",
      "mother\n",
      "son\n",
      "old\n",
      "woman\n",
      "boy\n",
      "girl\n",
      "house\n",
      "husband\n",
      "school\n",
      "home\n",
      "sister\n",
      "married\n",
      "adventure\n",
      "friends\n",
      "children\n",
      "gets\n",
      "day\n",
      "\n",
      "\n",
      "Topic 19\n",
      "\n",
      "didn\n",
      "re\n",
      "going\n",
      "nothing\n",
      "thing\n",
      "guy\n",
      "doesn\n",
      "things\n",
      "want\n",
      "actually\n",
      "anything\n",
      "got\n",
      "now\n",
      "pretty\n",
      "lot\n",
      "ll\n",
      "isn\n",
      "minutes\n",
      "someone\n",
      "right\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "widx_ranks=np.argsort(lda.components_,axis=1)\n",
    "\n",
    "numwords=20\n",
    "for i in np.arange(ntopics):\n",
    "    print(\"Topic {}\\n\".format(i))\n",
    "    for j in np.arange(numwords):\n",
    "        print(dictionary.get_feature_names()[widx_ranks[i,len(dictionary.vocabulary_)-j-1]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate the topic distributions assigned to the different reviews. These have first to be computed using the `transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "[[0.00131579 0.43100785 0.00131579 0.00131579 0.00131579 0.00131579\n",
      "  0.00131579 0.00131579 0.00131579 0.05414566 0.00131579 0.00131579\n",
      "  0.08458983 0.00131579 0.20784751 0.00131579 0.00131579 0.00131579\n",
      "  0.10238772 0.10160037]\n",
      " [0.00111111 0.00111111 0.49372067 0.00111111 0.00111111 0.00111111\n",
      "  0.00111111 0.00111111 0.00111111 0.00111111 0.03184332 0.00111111\n",
      "  0.15261789 0.00111111 0.00111111 0.00111111 0.00111111 0.00111111\n",
      "  0.00111111 0.30404034]\n",
      " [0.00238095 0.00238095 0.00238095 0.00238095 0.2980676  0.00238095\n",
      "  0.00238095 0.00238095 0.00238095 0.00238095 0.00238095 0.00238095\n",
      "  0.08461675 0.00238095 0.20732149 0.00238095 0.00238095 0.00238095\n",
      "  0.00238095 0.37189893]]\n"
     ]
    }
   ],
   "source": [
    "topicvecs = lda.transform(reviews_train_vec)\n",
    "\n",
    "print(topicvecs.shape)\n",
    "print(topicvecs[0:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the topic distributions as feature vectors to cluster the movies. The number of clusters we here choose can be quite different from the number of topics in the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster sizes:\n",
      "\n",
      "0: 604\n",
      "1: 551\n",
      "2: 481\n",
      "3: 257\n",
      "4: 623\n",
      "5: 333\n",
      "6: 607\n",
      "7: 382\n",
      "8: 1000\n",
      "9: 482\n",
      "10: 1007\n",
      "11: 578\n",
      "12: 954\n",
      "13: 486\n",
      "14: 427\n",
      "15: 458\n",
      "16: 604\n",
      "17: 479\n",
      "18: 446\n",
      "19: 911\n",
      "20: 1063\n",
      "21: 447\n",
      "22: 838\n",
      "23: 583\n",
      "24: 609\n",
      "25: 720\n",
      "26: 837\n",
      "27: 1006\n",
      "28: 450\n",
      "29: 419\n",
      "30: 954\n",
      "31: 321\n",
      "32: 487\n",
      "33: 635\n",
      "34: 590\n",
      "35: 486\n",
      "36: 670\n",
      "37: 477\n",
      "38: 954\n",
      "39: 784\n",
      "\n",
      "\n",
      "Silhouette score: 0.08037105092085178\n"
     ]
    }
   ],
   "source": [
    "numclus=40\n",
    "docclus=KMeans(n_clusters=numclus,n_init=1).fit(topicvecs)\n",
    "\n",
    "print(\"Cluster sizes:\\n\")\n",
    "for i in np.arange(numclus):\n",
    "    print(\"{}: {}\".format(i,len(np.where(docclus.labels_==i)[0])))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Silhouette score: {}\".format(silhouette_score(topicvecs,docclus.labels_)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quite uniform size distribution and the low Silhouette score indicate that we have not found a very clear or natural clustering. However, we can still use the cluster centers, and the reviews closest to the cluster centers as  representatives for different types of reviews.\n",
    "\n",
    "As before for the 'Faces' dataset, we use the `transform` method to compute a distance matrix of the datapoints to the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "(25000, 40)\n",
      "(25000, 40)\n"
     ]
    }
   ],
   "source": [
    "distances=docclus.transform(topicvecs)\n",
    "centrals=np.argsort(distances,axis=0)\n",
    "\n",
    "print(topicvecs.shape)\n",
    "print(distances.shape)\n",
    "print(centrals.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the reviews that are closest to the center of a given cluster. The following first prints the topic distribution vectors of the `norevs` reviews that are closest to the cluster center as columns in a matrix. The first column of the matrix just contains the indices of the topics. Then the actual reviews are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00e+00 2.25e-04 3.03e-04 6.79e-02]\n",
      " [1.00e+00 6.96e-02 6.70e-02 3.03e-02]\n",
      " [2.00e+00 5.91e-02 4.51e-02 6.09e-02]\n",
      " [3.00e+00 6.37e-02 6.22e-02 1.04e-01]\n",
      " [4.00e+00 2.25e-04 3.03e-04 6.35e-02]\n",
      " [5.00e+00 6.04e-02 6.94e-02 1.84e-04]\n",
      " [6.00e+00 1.91e-02 1.02e-02 1.84e-04]\n",
      " [7.00e+00 3.51e-02 1.77e-02 2.97e-02]\n",
      " [8.00e+00 2.25e-04 3.03e-04 9.73e-03]\n",
      " [9.00e+00 2.25e-04 3.59e-02 1.39e-02]\n",
      " [1.00e+01 1.41e-02 9.26e-03 1.87e-02]\n",
      " [1.10e+01 1.02e-01 3.03e-04 5.51e-02]\n",
      " [1.20e+01 2.25e-04 3.03e-04 1.84e-04]\n",
      " [1.30e+01 5.58e-02 9.43e-02 1.84e-04]\n",
      " [1.40e+01 3.47e-01 4.51e-01 3.31e-01]\n",
      " [1.50e+01 2.25e-04 1.46e-02 3.15e-02]\n",
      " [1.60e+01 1.44e-02 6.91e-02 1.18e-02]\n",
      " [1.70e+01 4.07e-02 3.50e-02 6.56e-02]\n",
      " [1.80e+01 5.03e-02 1.71e-02 4.68e-02]\n",
      " [1.90e+01 6.74e-02 3.03e-04 5.94e-02]]\n",
      "\n",
      "\n",
      "b'Film can be a looking glass to see the world in a new light. Good Night and Good Luck, for instance, offered parallels to modern judgement-without-evidence and encroachments on freedom. It is easier to examine a moral problem when it is not too close to home: by putting it in a fictional or historic context removed from our immediate situation. With pornography, we consider ourselves \\'enlightened\\' and our forefathers to be hidebound by quaint ideas - usually involving fire and brimstone, but the definition of what is obscene can easily fall prey to ignorance and unscientific interpretation instead of evidence. Bettie Page was a cult icon of an era which included not just McCarthyism but the banning of comics (such as Tales from the Crypt) on the basis that they would turn youths into half-mad juvenile delinquents. This film, developed from key questions raised in her life, poses dilemmas that are as relevant today as back in 1950.<br /><br />Our film opens with two key scenes. In the first, we see well-dressed, respectable looking men in a seedy bookshop. One of them asks for pictures of women in kinky boots and being restrained - then turns out to be an undercover cop conducting a sting. The second scene shows Bettie Page, waiting to be called as a witness, looking quite demure, as if she had just come out of church.<br /><br />The first 50mins are in black & white. Old fashioned film effects such as wipes and fades add to the sense that we are watching a film from bygone years, as do the mannerisms of the cast, skilfully recreated 1950s scenes, contemporaneous slang phrases and the terse dialogue associated with film-making of the period. Archive footage is frequently intercut - which will delight many and irritate others. In keeping with its theme, the movie is almost a collection of different types of photography-in-motion, and the older clips of fabulous beaches and landmarks juxtapose well against Bettie\\'s classic poses hearken back to an age of \\'health & nature\\' magazines . . . although I admit that if you are not captivated by the story you may find the effect a bit choppy.<br /><br />Very soon, we go into flashback. Bettie escapes a Depression Years downtrodden life in Nashville, enlivened only with church singing and soul-saving, and goes alone to make her own way. After her initial success, her modelling work splits into two strands: the mainstream glamour work focussing on her over-the-rainbow smile, and the \\'specialist interest\\' photos involving dressing up in high-heeled boots and light bondage gear. In an earlier audition (reminiscent of a scene with Naomi Watts\\' character in Mulholland Drive - who was also called Betty), she gives a performance that is full of emotion, contrasting with her normal animated, cheerful (but ultimately bland) day-to-day expression. Is part of her still unfulfilled? Bettie is frequently rejected in auditions once they realise she is the well-known pin-up girl. But we have never been asked to feel sorry for Bettie Page: her abusive childhood is quickly referenced and skipped over; when she is raped by four hometown lads, we see only the threat and then Bettie recovering and surviving, picking herself up in deserted woodland, and putting on the brave face of someone who refuses to lie down and die.<br /><br />Although no nudity is involved, it\\'s Bettie\\'s special interest photos that eventually arouse trouble. When we go back to the court, the opinions of a clergyman on the corrupting influence of such photos are taken as evidence. A psychologist authoritatively says how the photos lead to \\'suicide, murder and psychosis\\' in youngsters who are exposed to them (presumably not in that order). Eventually a star witness explains how his son\\'s life came to an end as a result of such photos - \\'being \\'trussed up like that\\'. It is not made clear how being \\'trussed up\\' causes death). The text accompanying a series of Bettie\\'s photographs in a magazine tell how she was forced to endure \\'terrible agonies\\' with the fetish restraints (the audience knows that she actually found them quite hilarious and that the wordings, like the photos themselves, were pure dramatisations). After a 12hr wait, Bettie is told her evidence - she is the one person who could state definitively that she was not photographed in agony - will \\'not be required\\'. The \\'horrors\\' of the photos have been proved.<br /><br />If this all sounds like the dark ages which we have left, consider a recent (2003) incident in which an Ann Summers advert was banned. It said, \"for fashion and passion whip along to your local store,\" with a photograph of a woman\\'s back. She\\'s wearing a bra and thong and her hands are handcuffed behind her back. The lingerie and sex toys company, that targets female consumers (and also supports charities fighting domestic violence) said its adverts aimed, \"to give women sexual confidence and always showed women in control of their sexuality.\" One might conclude that the prejudice and ignorance of the Betty Page investigations still holds currency.<br /><br />Bettie\\'s religious views are integral to the story, just as the concept of sin is integral to Christianity and contributes to the \\'forbidden\\' nature of sexual enjoyment frequently prevalent in the UK and US - as opposed to the more factual approach found in continental Europe. It could be argued the formulae of sin and redemption, and \"being saved\", are even reflected in mating patterns that perpetuate traditional male dominance. A policeman making a friendly (but sexually motivated) approach to Bettie outside the courtroom offers to \\'save\\' her from loneliness. The knight-in-shining-armour might be chivalrous, but it also assumes a woman in need of rescue.<br /><br />Love it or hate it, The Notorious Bettie Page is an unusual and extraordinary film, and a moral wake-up call for those that heed it. There is excellent ensemble acting, and Gretchen Mol, as Bettie, is remarkable as the whole film succeeds or falls on her powerful performance.'\n",
      "\n",
      "\n",
      "b'Many of the lead characters in Hideo Gosha\\'s 1969 film \"Hitokiri\" (manslayer; aka \"Tenchu\" -- heaven\\'s punishment) were actual historical figures (in \"western\" name-order format): Ryoma Sakamoto, Hampeita Takechi, Shimbei Tanaka, Izo Okada, ____ Anenokoji. The name \"Hitokiri,\" a historical term, refers to a group of four super-swordsmen who carried out numerous assassinations of key figures in the ruling Tokugawa Shogunate in the mid-1800s under the orders of Takechi, the leader of the \"Loyalist\" (i.e. ultra-nationalist, pro-Emperor) faction of the Tosa clan. What was this struggle about? Sad to say, you won\\'t find out in this film. \"Brilliant History Lesson\" indeed!<br /><br />No, Gosha is much more interested in showing you the usual bloody slicing and dicing and (at absurd length) the inner torment of the not-very-bright killer Izo Okada than in revealing actual history. Sakamoto, for example, was someone of historical significance, considered to be the father of the Imperial Japanese Navy. The closest Gosha comes to providing a history lesson is the scene in which Sakamoto, whom Takechi considers a traitor to the Loyalist cause, comes to Takechi\\'s mansion to try to sway him ideologically. He begins by talking about the international political situation, with foreign warships in Japan\\'s ports and a Japan that is too weak militarily to defend against them. Want to know more? Sorry. Gosha cuts off this potentially fascinating lecture in mid sentence(!). So much for informing his audience about a turning point in Japanese history.<br /><br />The film left me in utter confusion about the aims of the two sides in this struggle. For the two and a half centuries that the Shogunate held central power in Japan, it was an institution dedicated to preventing social change, to preserving the feudal relations of society. It was fearful of outside contamination, both ideological and technological. In keeping with this spirit, it outlawed firearms, those instruments of \"leveling\" in Europe and the Americas, with which a peasant could have stood up to a samurai. Throughout this period, the Emperor was nothing more than a spiritual figurehead.<br /><br />But, in the towns, which stood in neutral zones between the feudal fiefdoms, a new class of merchants, landlords and craftsmen was developing -- the class known in Europe by its French name, the bourgeoisie. Inevitably, as this new class gained strength, it chafed against the many confines of feudal society. As in Europe, the king (Emperor) became the central figure in the bourgeoisie\\'s struggle for power against the feudal aristocracy. But a political leadership does not always fully understand the interests of the class it serves. When the outside world arrived with a bang in 1853, in the form of U.S. Admiral Perry\\'s \"Black Ships,\" the ruling elite of Japan was thrown into a crisis. Their military was no match for these foreigners. Also, they had heard about the havoc the British and French imperialists were wreaking in China. What should Japan do to save itself from the fate of its weak neighbor? Surprisingly, some elements within the usually isolationist Shogunate were inclined to open trade with the foreigners in order to obtain some of their advanced technology. This is the point of view represented (just barely) in the film by Sakamoto. On the other hand, the Emperor-loyal ultra-nationalists, represented by Takechi, believed they could keep out the foreigners by force, if only they could prevent the other faction from \"selling out the country.\" (Sound familiar?) Thus, the assassination of key Shogunate figures is in order -- and away we go.<br /><br />Takechi\\'s motivations were, for me, the film\\'s biggest puzzle. Gosha suggests that he is fighting mainly for his personal advancement rather than for the Loyalist cause. Can we take this to represent the tenor of the Loyalists as a whole? (Do you care?)<br /><br />Several reviewers have compared this film favorably with \"Goyokin,\" which Gosha made in the same year. But, where \"Goyokin\" is a crackling, suspenseful, adventure yarn, with a hero worthy of sympathy, \"Hitokiri\" is plodding, nowhere near as compelling and lacks such a hero. Sakamoto could have been this film\\'s hero but we are not allowed to know him -- nor what he stands for -- well enough for him to achieve that status.<br /><br />In view of his wonderful scores for five previous Kurosawa films, Masaru Sato\\'s score here was very disappointing, sounding like something rejected from a \"Bonanza\" episode.<br /><br />Barry Freed'\n",
      "\n",
      "\n",
      "b'Oftentimes, films of this nature come across as a mixed bag of great work along with slight drivel to fill the runtime. Whether it be the big name support or the project itself, Paris je t\\'aime never falls into this realm. I believe I can truly say that the movie as a whole is better than its parts. Between the wonderful transitions and the fantastic ending sequence, merging characters together in one last view of love in Paris, I think the film would have suffered if any cog was removed. True, there are definitely a few standouts that overshadow the rest, but in the end I have a lasting image, even if just a split second of each short vignette. Love takes many forms, and the talent here rises to the occasion, to surprise and move the audience through shear poetry and elegance of the emotion\\'s many facets.<br /><br />Quartier des Enfants Rouges: Maggie Gyllenhaal surprises as a drug-addled actress shooting in Paris and meeting with her dealer. The reveal at its conclusion leaves you a bit off balance as the infatuation between the two changes hands.<br /><br />Quatier Latin: Ben Gazzara and Gena Rowlands (recreating a relationship from an old Cassavettes film?) bring some great sharp wit and sarcasm as they meet to discuss their impending divorce. What of their conversation is true and what is just to anger the other, it is all enjoyable, leaving a smile on your face.<br /><br />Quais de Seine: Director Gurinder Chadha gives us a touching portrait of love existing beyond religious and racial differences. It is a sweet little story of shy love between two people obviously feeling a connection, but unable to quite vocalize it.<br /><br />Tour Eiffel: I will admit to being disappointed that Sylvain Chomet did not get an animated sequence together, however, this live action tale of mimes falling in love at a Paris jail has the same quirky nature as his film Les Triplettes de Belleville.<br /><br />Tuileries: The Coen Brothers stick to their strange sense of humor and deliver some fine laughs. Steve Buscemi really shines and sells the performance without speaking a word. His facial reactions to the verbal abuse of a disgruntled Frenchman are priceless.<br /><br />Bastille: Here is a heartbreaking portrait of a couple about out of love only to have it come back in the face of tragedy. Sergio Castellitto and Miranda Richardson a moving as the couple dealing with trouble and finding how strong the bond of true love is.<br /><br />P\\xc3\\xa9re-Lachaise: A surprisingly funny little tale from horror master Wes Craven. A little Oscar Wilde humor can add levity to any relationship.<br /><br />Parc Monceau: Alfonso Cuar\\xc3\\xb3n looks to be practicing the amazing long-takes he perfects in Children of Men with this tale of two people in love, walking down the street. As Nick Nolte and Ludivine Sagnier eventually come into close-up view, we also find the true context of their conversation of \"forbidden love.\"<br /><br />Porte de Choisy: A very surreal look into the glamour of Paris. This is probably the most odd entry, but so intriguing that you can\\'t look away from the craziness that ensues. Do not anger your Asian beautician, whatever you do.<br /><br />Pigalle: An interesting look at a relationship undergoing a role-play that seems to have been stagnant for years. A little variety from Bob Hoskins is necessary to fire kindled.<br /><br />Quartier de la Madeleine: Even vampires in Paris can find love amongst the feeding hours. I don\\'t know whether to be happy for Elijah Wood as a result or not. Beautifully shot and muted to allow the vibrancy of the blood red, this short is strange, but then so is love.<br /><br />14th arrondissement: Leave it to Alexander Payne\\'s odd sense of humor to really add some depth to this voice-over story told of an American in Paris to find what love is. Her harsh, uneducated French is a very stark contrast to the authentic accents we\\'ve been listening to until this point\\xc2\\x97just off-kilter enough to be both funny and totally true to the story.<br /><br />Montmartre: An interesting introduction into the proceedings. Paris can be a city reviled for everyday activities like finding a parking spot, yet when love is discovered, it will take its prisoner anywhere to continue the journey.<br /><br />Loin du 16\\xc3\\xa9me: Catalina Sandino Moreno brilliantly shows what love for a child is through her subtle performance as the tale is bookended by her singing to a young child, yet totally different each time.<br /><br />Place des Fetes: My favorite tale of the bunch. Seydou Boro and A\\xc3\\xafssa Ma\\xc3\\xafga are simply fantastic. The cyclical nature of the story and how fate brings the two characters together twice in order for Boro to finally ask her for coffee is tough to watch. Sometimes love at your final moment is enough to accept one\\'s leaving of this earth.<br /><br />Place des Victoires: One of the best stories about a mother trying to cope with the death of her young son. Juliette Binoche is devastating as the mother, desperate for one last glimpse of her son, and Willem Dafoe is oddly perfect as the cowboy who allows her the chance.<br /><br />Faubourg Saint-Denis: Sometimes one needs to think he has lost love to accept that he has not been fully invested in the relationship. Melchior Beslon reminisces, trying to find where they went wrong through a series of sharp, quick cuts from his meeting Natalie Portman to eventually \"seeing\" how much he needs her.<br /><br />Le Marais: Leave it to Gus Van Sant to show us a story about the gap in communication and understanding as his films almost always deal with some form of alienation. His photographer from Elephant is an American working in Paris who is the catalyst for Gaspard Ulliel\\'s artist ramblings of love and soul mates. Sometimes one doesn\\'t need to know what is being said to understand what is going on in the pauses.'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The index of the cluster we want to investigate\n",
    "clusterno=6\n",
    "# The number of reviews closest to the cluster center we want to inspect\n",
    "norevs=3\n",
    "\n",
    "topicparams = np.zeros([ntopics,norevs+1])\n",
    "topicparams[:,0]=np.arange(ntopics)\n",
    "\n",
    "for i in np.arange(norevs):\n",
    "    topicparams[:,i+1]=topicvecs[centrals[i,clusterno],:]\n",
    " \n",
    "\n",
    "print(np.array_str(topicparams, precision=2))\n",
    "print(\"\\n\")\n",
    "\n",
    "#for j in np.arange(topicvecs.shape[1]):\n",
    "#    print(\"{}: {:.3}\".format(j,topicvecs[centrals[i,clusterno],j]))\n",
    "        \n",
    "for i in np.arange(norevs):    \n",
    "    print(reviews_train.data[centrals[i,clusterno]])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look which reviews are most strongly connected to any specific topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicvecs.shape\n",
    "toprevs_by_topic=np.argsort(topicvecs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'For those of you unfamiliar with Jimmy Stewart, this is one of his \"lesser\" films from later in his career. And, while it isn\\'t a great film compared to many of his other pictures, it isn\\'t bad and is a decent time-passer--but not much more.<br /><br />Kim Novak is a witch in New York City and for some inexplicable reason, she decides to cast a spell on poor Jimmy to make him fall in love with her. Over time, the cold and detached Ms. Novak also begins to fall in love with Stewart--and apparently in the witch\\'s rule book, this is a definite NO, NO!! <br /><br />The film is odd in its sensibilities about the witches. They are neither the baby-sacrificing nor the all-powerful variety. Most of their magic is pretty limited and pointless (such as Jack Lemmon using his powers to turn off street lamps). And, very oddly, the witches all seem to be bohemians who hang out in hip bars where you might find people wearing berets and listening to crappy jazz. Considering what I think of jazz, it must really stink to be a witch in this movie\\'s world!<br /><br />Anyway, the film is pretty romantic and mildly comedic, but not something I would rush out to watch. The acting is pretty good, but the script doesn\\'t offer enough payoff to make this an exceptional film--in fact, I almost scored the film a 6--it was really close.'\n",
      "\n",
      "\n",
      "b\"The latest Rumor going around is that Vh1 is starting casting calls for I Love New York 3 mid 2008. So does this mean Budah or Tailor made dumped New York or does this mean New York dumped the winner?<br /><br />I know Flavor of Love is coming up to it's 3rd season, so now with a Flavor of Love 3 and a I love New York 3.....will there ever be a true winner???<br /><br />I've also heard a few rumors that Chance WILL be brought back for the 3rd Season of I Love New York!!!! I have also heard rumors that New York will be Specially featured on Flavor of Love 3. <br /><br />Hopefully this was not too much of a spoiler for the ending of I Love New York 2....I'm just stating the latest rumor.\"\n",
      "\n",
      "\n",
      "b'Thanks to this film, I now can answer the question, \"What is the worst movie you have ever seen?\"<br /><br />I can\\'t even think of a close second, and I\\'ve seen some really bad movies.<br /><br />Absolutely nothing works in this film. Name a single element of any horror film and this movie fails. Honestly, I\\'ve seen better on YouTube. Here\\'s some typical dialogue:<br /><br />\"Steve?\" \"Steve?\" \"Steve, is that you?\" \"Steve, I\\'m not kidding\" \"Steve, this isn\\'t funny!\" \"Steve, are you there?\" \"Steve?\" \"Steve?\" \"Steve?\"<br /><br />\"ARggh!!!! Ahhhhhh!!!! Nooooooo!\"'\n",
      "\n",
      "\n",
      "b\"I think it's one of the greatest movies which are ever made, and I've seen many... The book is better, but it's still a very good movie!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The index of the topic we want to investigate\n",
    "topicno=8\n",
    "# The number of reviews most highly associated with the topic that we want to see\n",
    "norevs=4\n",
    "\n",
    "l=len(toprevs_by_topic)-1\n",
    "\n",
    "for i in np.arange(norevs):    \n",
    "    print(reviews_train.data[toprevs_by_topic[l-i,topicno]])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
