{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we perform character recognition using SVM and NN classifiers. We use the MNIST dataset, which consists of 70000 handwritten digits 0..9 at a resolution of 28x28 pixels. \n",
    "\n",
    "Stuff we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn as skl\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the MNIST data. Using the fetch_mldata function, this will be downloaded from the web, and stored in the directory you specify as data_home (replace my path in the following cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(name='mnist_784', data_home='/home/kenneth/Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has .data and .target attributes. The following gives us some basic information on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 70000\n",
      "\n",
      "Number of features: 784\n",
      "\n",
      "List of labels: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datapoints: {}\\n\".format(mnist.data.shape[0]))\n",
    "print(\"Number of features: {}\\n\".format(mnist.data.shape[1]))\n",
    "print(\"List of labels: {}\\n\".format(np.unique(mnist.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist.data is represented as a Pandas dataframe. The following code expects mnist.data to be a plain np.array, which we get simply by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data=np.array(mnist.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot individual datapoints as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of datapoint no. 4:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  55 148 210 253 253 113  87 148\n",
      "  55   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  87 232 252 253 189 210 252 252 253 168   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   4  57 242 252 190  65   5  12 182\n",
      " 252 253 116   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  96 252 252 183  14   0   0  92 252 252 225  21   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 132 253 252 146  14   0   0   0\n",
      " 215 252 252  79   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 126 253 247 176   9   0   0   8  78 245 253 129   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  16 232 252 176   0   0   0  36\n",
      " 201 252 252 169  11   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  22 252 252  30  22 119 197 241 253 252 251  77   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  16 231 252 253 252 252\n",
      " 252 226 227 252 231   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  55 235 253 217 138  42  24 192 252 143   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  62 255 253 109   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  71 253 252  21   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 253 252  21   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  71 253 252\n",
      "  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 106 253 252  21   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  45\n",
      " 255 253  21   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 218 252  56   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  96 252 189  42   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  14 184 252 170  11   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  14 147 252  42   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "\n",
      "Target of datapoint no. 4:\n",
      "9\n",
      "\n",
      "As image:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "print(\"Value of datapoint no. {}:\\n{}\\n\".format(index,mnist.data[index,:]))\n",
    "print(\"Target of datapoint no. {}:\\n{}\\n\".format(index,mnist.target[index])) ## Added the target value of index 4\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(mnist.data[index].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little bit simpler (and faster!), we can extract from the data binary subsets, that only contain the data for two selected digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first datapoint now is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7        3\n",
      "10       3\n",
      "12       3\n",
      "15       7\n",
      "27       3\n",
      "        ..\n",
      "69975    3\n",
      "69979    7\n",
      "69986    3\n",
      "69990    7\n",
      "69996    3\n",
      "Name: class, Length: 14434, dtype: category\n",
      "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "digit0='3'\n",
    "digit1='7'\n",
    "mnist_bin_data=mnist.data[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "mnist_bin_target=mnist.target[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "print(\"The first datapoint now is: \\n\")\n",
    "plt.imshow(mnist_bin_data[0].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()\n",
    "print(mnist_bin_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Split the mnist_bin data into training and test set. Learn different SVM and NN models by varying the kernel functions (SVM), the network structure (NN), and the solver (NN). For each configuration, determine the time it takes to learn the model, and the accuracy on the test data. *Caution*: for some configurations, learning here can take a little while (several minutes).\n",
    "\n",
    "Using the numpy where() function, one can extract the indices of the test cases that were misclassified: <br>\n",
    "`misclass = np.where(test != predictions)` <br>\n",
    "Inspect some misclassified cases. Do they correspond to hard to recognize digits (also for the human reader)? \n",
    "\n",
    "How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full classification of the 10 classes in the original dataset (after split into train/test). Using `sklearn.metrics.confusion_matrix` you can get an overview of all combinations of true and predicted labels (see p. 298-299 in Müller & Guido). What does this tell you about which digits are easy, and which ones are difficult to recognize, and which ones are most easily confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Split the mnist_bin data into training and test set. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Splitting the data set into a test and train. This only holds 3 and 7, as per split in previous section\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_bin_data, mnist_bin_target, random_state=1) ## test size 25% and training 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to learn the linear kernel model: 3.36 seconds\n",
      "Training Accuracy: 1.00\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "## Exercise: Learn different SVM and NN models by varying the kernel functions (SVM), the network structure (NN), and the solver (NN)\n",
    "import time\n",
    "\n",
    "## In the below section we learn a SVM model with a linear kernel type. We measure the time it takes to learn the model (as is done in the coming sections as well) to know\n",
    "## how long it takes and hence include that when evaluating the performance of the model.\n",
    "\n",
    "kernel_type = 'linear'\n",
    "start=time.time()\n",
    "linear_svm = SVC(kernel=kernel_type).fit(X_train,y_train)\n",
    "end=time.time()\n",
    "elapsed_time=end-start \n",
    "print(f\"Time it took to learn the {kernel_type} kernel model: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = linear_svm.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = linear_svm.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM linear model is done in 4 seconds and has a test accuracy of 98% and 100% on the training set.\n",
    "\n",
    "This suggest a model which overfits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it took to learn the rbf kernel model: 131.57 seconds\n",
      "Training Accuracy: 1.00\n",
      "Test Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "## Exercise: Learn different SVM and NN models by varying the kernel functions (SVM), the network structure (NN), and the solver (NN)\n",
    "import time\n",
    "\n",
    "## In the next section we learn a SVM model, this time with a rbf kernel type. Gamma is set to 0.1 and not altered during this exercise.\n",
    "\n",
    "\n",
    "kernel_type = 'rbf'\n",
    "start=time.time()\n",
    "kernel_svm = SVC(kernel=kernel_type, gamma=0.1).fit(X_train,y_train)\n",
    "end=time.time()\n",
    "elapsed_time=end-start \n",
    "print(f\"Time it took to learn the {kernel_type} kernel model: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = kernel_svm.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = kernel_svm.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model using the rbf kernel takes 133 seconds and gets 51% in accuracy on the test data and 100% on the training data.\n",
    "\n",
    "Comparing the two SVM the latter (kernel = rbf) performs poorly (51%) compared to the first (kernel = linear) (98%). Also the latter takes more than 2 min compared to only 4 seconds in the first model.\n",
    "\n",
    "Again we see a 100% fit on training data, which again suggest overfitting. With 51% on test data, the models does not have a great generalizability.\n",
    "\n",
    "Since the linear kernel outperforms the rbf it suggests that the problem has more of a linear nature.\n",
    "\n",
    "Next section turns to neural networks - let's see how they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This next section is rather expansive, so let's run through the intention.\n",
    "\n",
    "The overall purpose is to define multiple NN configurations, including changing the solver.\n",
    "\n",
    "Neural networks can have 0 or more hidden layers. For each layer it's possible to configure the number of nodes. E.g. () denotes no hidden layers, (10, 10, 10) denotes three layers each layer containing to nodes. The construction of the layers and number of nodes is it's architecture.\n",
    "Furthermore it possible to set the activation to a number of variations. Here relu and tanh is used.\n",
    "The NN can be configured with different solvers, here also provided as input parameter using lbfgs and adam.\n",
    "To find a well performing model, several iterates are run through in order to find a good (valley) mode. The number of iterations can be set and impacts the model ability to find a convergence point. A model converges once its ability to improve the loss function decreases, meaning that from one iteration to the next there is very little or no improvement in the loss function. The loss function measured the model ability to predict versus the true target value. For each iteration a model is devised and it used to predict an outcome. The outcome of the prediction is then compared to the actual true value. In our case, this would amount to the model being learned and a data point being provided for the model to predict the outcome. If it predicts 3 and the actual true value is 7, then the prediction is incorrect. The loss function is impacted. In the next iteration it predicts 7 and the true value is 7, the loss function is better and hence there is a change from first to next iteration. At some point the models ability to predict changes little and the loss function changes is therefore little. The model has converged. \n",
    "A model can run out of iterations without being able to converge, which results in a convergence warning. Iterations are set to 100, 500, and 1000. In case a model converges on 100 iterations, it does not try 500 and 1000. It case it does not converge, it tries next level of iterations. \n",
    "\n",
    "There is a range of possible configurations and architectures, leaving a number of possible scenarios. The code below build a iterative model, that can be configured to run through different scenarios. The code model iterates through the defined architecture, solvers, iterations and activations. Firstly it tries each architecture (e.g. () then (5,), then (10,) and so on) with each activations.\n",
    "\n",
    "The resulting output is saved in a csv file with related information about the model e.g. the architecture, the activation, the number of iterations, the solver, the activation, time to learn the model as so on. It appends the result, meaning when the code is run multiple times, previous results are not overwritten.\n",
    "\n",
    "While it may seem like extensive code to try different models, this is to automate the task and make it easy to try different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting configuration: Arch=(), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:00, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:00, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5,), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:14, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:00:19, Converged: True\n",
      "    Model converged at max_iter=1000, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5,), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:16, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5,), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:05, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5,), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20,), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20,), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:29, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:01:23, Converged: False\n",
      "\n",
      "Starting configuration: Arch=(20,), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20,), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:17, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:00:32, Converged: False\n",
      "\n",
      "Starting configuration: Arch=(5, 5), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:04, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:06, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20, 20), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20, 20), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:04, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:29, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:01:08, Converged: False\n",
      "\n",
      "Starting configuration: Arch=(20, 20), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(20, 20), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:07, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5, 5), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5, 5), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:18, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:00:30, Converged: True\n",
      "    Model converged at max_iter=1000, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5, 5), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:04, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:04, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(5, 5, 5), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:12, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:23, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:00:41, Converged: False\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:05, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10, 10), Solver=lbfgs, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:04, Converged: True\n",
      "    Model converged at max_iter=500, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10, 10), Solver=lbfgs, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:04, Converged: False\n",
      "  Testing max_iter=500...\n",
      "    Elapsed Time: 00:00:25, Converged: False\n",
      "  Testing max_iter=1000...\n",
      "    Elapsed Time: 00:00:53, Converged: False\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10, 10), Solver=adam, Activation=relu\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:03, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "Starting configuration: Arch=(10, 10, 10, 10), Solver=adam, Activation=tanh\n",
      "  Testing max_iter=100...\n",
      "    Elapsed Time: 00:00:02, Converged: True\n",
      "    Model converged at max_iter=100, skipping remaining iterations for this configuration.\n",
      "\n",
      "All configurations complete. Results stored in 'nn_results.csv'.\n",
      "\n",
      "Final Results Summary:\n",
      "    Architecture Solver  Max_Iterations Activation  Train_Accuracy  Test_Accuracy  Converged Elapsed_Time\n",
      "              ()  lbfgs             100       relu             NaN            NaN      False     00:00:00\n",
      "              ()  lbfgs             500       relu        1.000000       0.984206       True     00:00:02\n",
      "              ()  lbfgs             100       tanh             NaN            NaN      False     00:00:00\n",
      "              ()  lbfgs             500       tanh        1.000000       0.984206       True     00:00:03\n",
      "              ()   adam             100       relu        0.992517       0.985037       True     00:00:02\n",
      "              ()   adam             100       tanh        0.992517       0.985037       True     00:00:02\n",
      "            (5,)  lbfgs             100       relu             NaN            NaN      False     00:00:02\n",
      "            (5,)  lbfgs             500       relu             NaN            NaN      False     00:00:14\n",
      "            (5,)  lbfgs            1000       relu        0.991686       0.990856       True     00:00:19\n",
      "            (5,)  lbfgs             100       tanh             NaN            NaN      False     00:00:03\n",
      "            (5,)  lbfgs             500       tanh        0.989654       0.988640       True     00:00:16\n",
      "            (5,)   adam             100       relu             NaN            NaN      False     00:00:03\n",
      "            (5,)   adam             500       relu        0.995658       0.988917       True     00:00:05\n",
      "            (5,)   adam             100       tanh        0.988637       0.991687       True     00:00:03\n",
      "           (20,)  lbfgs             100       relu        1.000000       0.995844       True     00:00:05\n",
      "           (20,)  lbfgs             100       tanh             NaN            NaN      False     00:00:05\n",
      "           (20,)  lbfgs             500       tanh             NaN            NaN      False     00:00:29\n",
      "           (20,)  lbfgs            1000       tanh             NaN            NaN      False     00:01:23\n",
      "           (20,)   adam             100       relu        0.999353       0.991410       True     00:00:03\n",
      "           (20,)   adam             100       tanh        0.988360       0.989194       True     00:00:05\n",
      "          (5, 5)  lbfgs             100       relu        0.987529       0.989194       True     00:00:02\n",
      "          (5, 5)  lbfgs             100       tanh             NaN            NaN      False     00:00:03\n",
      "          (5, 5)  lbfgs             500       tanh             NaN            NaN      False     00:00:17\n",
      "          (5, 5)  lbfgs            1000       tanh             NaN            NaN      False     00:00:32\n",
      "          (5, 5)   adam             100       relu             NaN            NaN      False     00:00:04\n",
      "          (5, 5)   adam             500       relu        0.998245       0.990579       True     00:00:06\n",
      "          (5, 5)   adam             100       tanh        0.982540       0.983375       True     00:00:03\n",
      "        (20, 20)  lbfgs             100       relu        1.000000       0.994458       True     00:00:02\n",
      "        (20, 20)  lbfgs             100       tanh             NaN            NaN      False     00:00:04\n",
      "        (20, 20)  lbfgs             500       tanh             NaN            NaN      False     00:00:29\n",
      "        (20, 20)  lbfgs            1000       tanh             NaN            NaN      False     00:01:08\n",
      "        (20, 20)   adam             100       relu        0.999815       0.990302       True     00:00:03\n",
      "        (20, 20)   adam             100       tanh        0.991963       0.989471       True     00:00:07\n",
      "       (5, 5, 5)  lbfgs             100       relu             NaN            NaN      False     00:00:02\n",
      "       (5, 5, 5)  lbfgs             500       relu        0.998337       0.989471       True     00:00:02\n",
      "       (5, 5, 5)  lbfgs             100       tanh             NaN            NaN      False     00:00:03\n",
      "       (5, 5, 5)  lbfgs             500       tanh             NaN            NaN      False     00:00:18\n",
      "       (5, 5, 5)  lbfgs            1000       tanh        0.989931       0.987531       True     00:00:30\n",
      "       (5, 5, 5)   adam             100       relu             NaN            NaN      False     00:00:04\n",
      "       (5, 5, 5)   adam             500       relu        0.998430       0.993627       True     00:00:04\n",
      "       (5, 5, 5)   adam             100       tanh        0.988730       0.987808       True     00:00:02\n",
      "    (10, 10, 10)  lbfgs             100       relu             NaN            NaN      False     00:00:02\n",
      "    (10, 10, 10)  lbfgs             500       relu        0.999815       0.996398       True     00:00:12\n",
      "    (10, 10, 10)  lbfgs             100       tanh             NaN            NaN      False     00:00:05\n",
      "    (10, 10, 10)  lbfgs             500       tanh             NaN            NaN      False     00:00:23\n",
      "    (10, 10, 10)  lbfgs            1000       tanh             NaN            NaN      False     00:00:41\n",
      "    (10, 10, 10)   adam             100       relu        0.999538       0.995290       True     00:00:05\n",
      "    (10, 10, 10)   adam             100       tanh        0.980416       0.980050       True     00:00:05\n",
      "(10, 10, 10, 10)  lbfgs             100       relu             NaN            NaN      False     00:00:03\n",
      "(10, 10, 10, 10)  lbfgs             500       relu        1.000000       0.993627       True     00:00:04\n",
      "(10, 10, 10, 10)  lbfgs             100       tanh             NaN            NaN      False     00:00:04\n",
      "(10, 10, 10, 10)  lbfgs             500       tanh             NaN            NaN      False     00:00:25\n",
      "(10, 10, 10, 10)  lbfgs            1000       tanh             NaN            NaN      False     00:00:53\n",
      "(10, 10, 10, 10)   adam             100       relu        0.999908       0.993073       True     00:00:03\n",
      "(10, 10, 10, 10)   adam             100       tanh        0.987714       0.988362       True     00:00:02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Configurations\n",
    "architectures = [(), (5,), (20,), (5, 5), (20, 20), (5, 5, 5), (10, 10, 10), (10, 10, 10, 10)]\n",
    "solvers = ['lbfgs', 'adam']\n",
    "activations = ['relu', 'tanh']\n",
    "iterations = [100, 500, 1000, 5000]\n",
    "random_state = 11  # Fixed random state for reproducibility\n",
    "\n",
    "# Output CSV file\n",
    "output_file = 'nn_results.csv'\n",
    "\n",
    "# Check if file exists to append results\n",
    "file_exists = os.path.isfile(output_file)\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Start iterating over all configurations\n",
    "for architecture in architectures:\n",
    "    for solver in solvers:\n",
    "        for activation in activations:\n",
    "            print(f\"\\nStarting configuration: Arch={architecture}, Solver={solver}, Activation={activation}\")\n",
    "            \n",
    "            for max_iter in iterations:\n",
    "                # Print current model run\n",
    "                print(f\"  Testing max_iter={max_iter}...\")\n",
    "                \n",
    "                # Initialize the model\n",
    "                mlp = MLPClassifier(hidden_layer_sizes=architecture,\n",
    "                                    solver=solver,\n",
    "                                    activation=activation,\n",
    "                                    max_iter=max_iter,\n",
    "                                    random_state=random_state)\n",
    "                \n",
    "                # Measure time\n",
    "                start_time = time.time()\n",
    "                converged = True\n",
    "                \n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"error\", category=ConvergenceWarning)\n",
    "                    try:\n",
    "                        # Train the model\n",
    "                        mlp.fit(X_train, y_train)\n",
    "                    except ConvergenceWarning:\n",
    "                        converged = False\n",
    "                    except Exception as e:\n",
    "                        print(f\"Unexpected error: {e}\")\n",
    "                        converged = False\n",
    "                \n",
    "                # Measure elapsed time\n",
    "                elapsed_time = time.time() - start_time\n",
    "                elapsed_hhmmss = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "                \n",
    "                # Calculate accuracies only if converged\n",
    "                train_accuracy = mlp.score(X_train, y_train) if converged else None\n",
    "                test_accuracy = mlp.score(X_test, y_test) if converged else None\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'Architecture': architecture,\n",
    "                    'Solver': solver,\n",
    "                    'Max_Iterations': max_iter,\n",
    "                    'Activation': activation,\n",
    "                    'Random_State': random_state,\n",
    "                    'Train_Accuracy': train_accuracy,\n",
    "                    'Test_Accuracy': test_accuracy,\n",
    "                    'Converged': converged,\n",
    "                    'Elapsed_Time': elapsed_hhmmss\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "\n",
    "                # Print results for this run\n",
    "                print(f\"    Elapsed Time: {elapsed_hhmmss}, Converged: {converged}\")\n",
    "                \n",
    "                # If the model converged, stop testing further iterations\n",
    "                if converged:\n",
    "                    print(f\"    Model converged at max_iter={max_iter}, skipping remaining iterations for this configuration.\")\n",
    "                    break\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Append results to CSV file\n",
    "if file_exists:\n",
    "    results_df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "else:\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Completion message\n",
    "print(\"\\nAll configurations complete. Results stored in 'nn_results.csv'.\")\n",
    "\n",
    "# Print results in a table format\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(results_df[['Architecture', 'Solver', 'Max_Iterations', 'Activation',\n",
    "                  'Train_Accuracy', 'Test_Accuracy', 'Converged', 'Elapsed_Time']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code three NN architectures are run through and misclassifications examined. Since the code above iterates through multiple models, using the last available model to see misclassifications seems a little unfit. Therefore three models that has performed well previously in terms of time and precision, are used and misclassifications for all three are reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: ()\n",
      "Number of Misclassifications: 57\n",
      "Indices of Misclassifications: [ 43  61  70  72 110 276 349 392 413 421]\n",
      "--------------------------------------------------\n",
      "Architecture: (20,)\n",
      "Number of Misclassifications: 19\n",
      "Indices of Misclassifications: [ 276  392  637  717 1055 1097 1117 1413 1488 1729]\n",
      "--------------------------------------------------\n",
      "Architecture: (5, 5, 5)\n",
      "Number of Misclassifications: 37\n",
      "Indices of Misclassifications: [  43   72  392  479  624  707  717 1007 1029 1056]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Exercise: Using the numpy where() function, one can extract the indices of the test cases that were misclassified: <br> `misclass = np.where(test != predictions)` <br>\n",
    "## Exercise: Inspect some misclassified cases. Do they correspond to hard to recognize digits (also for the human reader)? \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## Three different architectures are examined 1) no layers, 2) one layer with 20 nodes and 3) three layers with 5 nodes in each\n",
    "architectures = [(), (20,), (5, 5, 5)]\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 12))  # Adjust grid size\n",
    "\n",
    "# Iterate over each architecture\n",
    "for i, architecture in enumerate(architectures):\n",
    "    # Create and train the neural network model\n",
    "    model = MLPClassifier(hidden_layer_sizes=architecture, activation='relu', solver='lbfgs', max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Find misclassifications\n",
    "    misclassified_indices = np.where(y_pred != y_test)[0]\n",
    "    num_misclassified = len(misclassified_indices)\n",
    "    misclassified_indices_list = misclassified_indices[:10]  # Limit to the first 10\n",
    "    \n",
    "    # Print misclassification details\n",
    "    print(f\"Architecture: {architecture}\")\n",
    "    print(f\"Number of Misclassifications: {num_misclassified}\")\n",
    "    print(f\"Indices of Misclassifications: {misclassified_indices_list}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display the architecture and misclassification info\n",
    "    axes[i, 0].set_title(f\"Architecture: {architecture}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    misclass_text = f\"Misclassified: {num_misclassified}\\nIndices: {misclassified_indices_list}\"\n",
    "    axes[i, 0].text(0.5, 0.5, misclass_text, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Plot the first 5 misclassified images\n",
    "    for j in range(min(5, num_misclassified)):\n",
    "        # Get the misclassified image index\n",
    "        index = misclassified_indices[j]\n",
    "        \n",
    "        # Get the image and reshape it\n",
    "        image = X_test[index].reshape(28, 28)  # Direct indexing for NumPy arrays\n",
    "        \n",
    "        # Plot the image\n",
    "        ax = axes[i, j+1]  # Adjusted column indexing\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Use .iloc[] to properly index into y_test if it's a Pandas Series\n",
    "        true_label = y_test.iloc[index] if isinstance(y_test, pd.Series) else y_test[index]\n",
    "        pred_label = y_pred[index]\n",
    "        \n",
    "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\")  # Show true and predicted labels\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three architectures perform differently. The architecture with one layer and 20 nodes has fewer misclassifications (19) compared to the two others (57 and 37 respectively)\n",
    "\n",
    "All of the models have misclassifications where it can be understood why it predicts incorrectly, while I would probably say that a human could have predicted most of these correctly. This of course only shows the misclassifications and there could be cases where the model predicted correctly, and a human would have struggled - we do not gain any insights into that in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first datapoint now is: \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3' '3' '3' ... '3' '7' '3']\n",
      "Time it took to learn the linear kernel model: 3.32 seconds\n",
      "Training Accuracy: 1.00\n",
      "Test Accuracy: 0.98\n",
      "Testing architecture: () with activation: relu\n",
      "Start time is:  10:50:09\n",
      "End time for model training is:  10:50:11\n",
      "Before I move on time is:  10:50:11\n",
      "Testing architecture: (5,) with activation: relu\n",
      "Start time is:  10:50:11\n",
      "End time for model training is:  10:50:27\n",
      "Before I move on time is:  10:50:27\n",
      "Testing architecture: (10,) with activation: relu\n",
      "Start time is:  10:50:27\n",
      "End time for model training is:  10:50:31\n",
      "Before I move on time is:  10:50:31\n",
      "Testing architecture: (15,) with activation: relu\n",
      "Start time is:  10:50:31\n",
      "End time for model training is:  10:50:36\n",
      "Before I move on time is:  10:50:36\n",
      "Testing architecture: (20,) with activation: relu\n",
      "Start time is:  10:50:36\n",
      "End time for model training is:  10:50:39\n",
      "Before I move on time is:  10:50:39\n",
      "Testing architecture: (5, 5) with activation: relu\n",
      "Start time is:  10:50:39\n",
      "End time for model training is:  10:50:41\n",
      "Before I move on time is:  10:50:41\n",
      "Testing architecture: (10, 5) with activation: relu\n",
      "Start time is:  10:50:41\n",
      "End time for model training is:  10:50:48\n",
      "Before I move on time is:  10:50:48\n",
      "Testing architecture: (5, 5, 5) with activation: relu\n",
      "Start time is:  10:50:48\n",
      "End time for model training is:  10:50:51\n",
      "Before I move on time is:  10:50:51\n",
      "Testing architecture: (10, 10, 5) with activation: relu\n",
      "Start time is:  10:50:51\n",
      "End time for model training is:  10:50:55\n",
      "Before I move on time is:  10:50:56\n",
      "Testing architecture: (10, 10, 10) with activation: relu\n",
      "Start time is:  10:50:56\n",
      "End time for model training is:  10:51:11\n",
      "Before I move on time is:  10:51:11\n",
      "Testing architecture: (15, 15, 15) with activation: relu\n",
      "Start time is:  10:51:11\n",
      "End time for model training is:  10:51:16\n",
      "Before I move on time is:  10:51:16\n",
      "Testing architecture: (5, 5, 5, 5) with activation: relu\n",
      "Start time is:  10:51:16\n",
      "End time for model training is:  10:51:37\n",
      "Before I move on time is:  10:51:37\n",
      "Testing architecture: (10, 10, 10, 10) with activation: relu\n",
      "Start time is:  10:51:37\n",
      "End time for model training is:  10:51:42\n",
      "Before I move on time is:  10:51:42\n",
      "Testing architecture: () with activation: tanh\n",
      "Start time is:  10:51:42\n",
      "End time for model training is:  10:51:45\n",
      "Before I move on time is:  10:51:45\n",
      "Testing architecture: (5,) with activation: tanh\n",
      "Start time is:  10:51:45\n",
      "End time for model training is:  10:52:07\n",
      "Before I move on time is:  10:52:07\n",
      "Testing architecture: (10,) with activation: tanh\n",
      "Start time is:  10:52:07\n",
      "End time for model training is:  10:53:11\n",
      "Before I move on time is:  10:53:11\n",
      "Testing architecture: (15,) with activation: tanh\n",
      "Start time is:  10:53:11\n",
      "End time for model training is:  10:54:20\n",
      "Before I move on time is:  10:54:20\n",
      "Testing architecture: (20,) with activation: tanh\n",
      "Start time is:  10:54:20\n",
      "End time for model training is:  10:55:21\n",
      "Before I move on time is:  10:55:22\n",
      "Testing architecture: (5, 5) with activation: tanh\n",
      "Start time is:  10:55:22\n",
      "End time for model training is:  10:56:00\n",
      "Before I move on time is:  10:56:00\n",
      "Testing architecture: (10, 5) with activation: tanh\n",
      "Start time is:  10:56:00\n",
      "End time for model training is:  10:58:36\n",
      "Before I move on time is:  10:58:36\n",
      "Testing architecture: (5, 5, 5) with activation: tanh\n",
      "Start time is:  10:58:36\n",
      "End time for model training is:  10:59:14\n",
      "Before I move on time is:  10:59:14\n",
      "Testing architecture: (10, 10, 5) with activation: tanh\n",
      "Start time is:  10:59:14\n",
      "End time for model training is:  11:01:37\n",
      "Before I move on time is:  11:01:37\n",
      "Testing architecture: (10, 10, 10) with activation: tanh\n",
      "Start time is:  11:01:37\n",
      "End time for model training is:  11:04:33\n",
      "Before I move on time is:  11:04:33\n",
      "Testing architecture: (15, 15, 15) with activation: tanh\n",
      "Start time is:  11:04:33\n",
      "End time for model training is:  11:06:52\n",
      "Before I move on time is:  11:06:52\n",
      "Testing architecture: (5, 5, 5, 5) with activation: tanh\n",
      "Start time is:  11:06:52\n",
      "End time for model training is:  11:07:45\n",
      "Before I move on time is:  11:07:45\n",
      "Testing architecture: (10, 10, 10, 10) with activation: tanh\n",
      "Start time is:  11:07:45\n",
      "End time for model training is:  11:10:35\n",
      "Before I move on time is:  11:10:36\n",
      "Results saved to mlp_results.csv\n",
      "The results:          architecture activation solver  iterations  random_state  \\\n",
      "0                 ()       relu  lbfgs        5000            11   \n",
      "1               (5,)       relu  lbfgs        5000            11   \n",
      "2              (10,)       relu  lbfgs        5000            11   \n",
      "3              (15,)       relu  lbfgs        5000            11   \n",
      "4              (20,)       relu  lbfgs        5000            11   \n",
      "5             (5, 5)       relu  lbfgs        5000            11   \n",
      "6            (10, 5)       relu  lbfgs        5000            11   \n",
      "7          (5, 5, 5)       relu  lbfgs        5000            11   \n",
      "8        (10, 10, 5)       relu  lbfgs        5000            11   \n",
      "9       (10, 10, 10)       relu  lbfgs        5000            11   \n",
      "10      (15, 15, 15)       relu  lbfgs        5000            11   \n",
      "11      (5, 5, 5, 5)       relu  lbfgs        5000            11   \n",
      "12  (10, 10, 10, 10)       relu  lbfgs        5000            11   \n",
      "13                ()       tanh  lbfgs        5000            11   \n",
      "14              (5,)       tanh  lbfgs        5000            11   \n",
      "15             (10,)       tanh  lbfgs        5000            11   \n",
      "16             (15,)       tanh  lbfgs        5000            11   \n",
      "17             (20,)       tanh  lbfgs        5000            11   \n",
      "18            (5, 5)       tanh  lbfgs        5000            11   \n",
      "19           (10, 5)       tanh  lbfgs        5000            11   \n",
      "20         (5, 5, 5)       tanh  lbfgs        5000            11   \n",
      "21       (10, 10, 5)       tanh  lbfgs        5000            11   \n",
      "22      (10, 10, 10)       tanh  lbfgs        5000            11   \n",
      "23      (15, 15, 15)       tanh  lbfgs        5000            11   \n",
      "24      (5, 5, 5, 5)       tanh  lbfgs        5000            11   \n",
      "25  (10, 10, 10, 10)       tanh  lbfgs        5000            11   \n",
      "\n",
      "    train_accuracy  test_accuracy        time formatted_time converged  \\\n",
      "0         1.000000       0.984206    2.663032       0m 2.66s       Yes   \n",
      "1         0.991686       0.990856   15.541131      0m 15.54s       Yes   \n",
      "2         0.999815       0.993350    3.808351       0m 3.81s       Yes   \n",
      "3         1.000000       0.995012    5.368036       0m 5.37s       Yes   \n",
      "4         1.000000       0.995844    2.629629       0m 2.63s       Yes   \n",
      "5         0.987529       0.989194    2.020052       0m 2.02s       Yes   \n",
      "6         0.999076       0.991133    6.545460       0m 6.55s       Yes   \n",
      "7         0.998337       0.989471    2.742559       0m 2.74s       Yes   \n",
      "8         0.999630       0.993627    4.741474       0m 4.74s       Yes   \n",
      "9         0.999815       0.996398   15.350420      0m 15.35s       Yes   \n",
      "10        1.000000       0.995290    5.258553       0m 5.26s       Yes   \n",
      "11        0.978753       0.980327   20.673042      0m 20.67s       Yes   \n",
      "12        1.000000       0.993627    4.926589       0m 4.93s       Yes   \n",
      "13        1.000000       0.984206    2.809197       0m 2.81s       Yes   \n",
      "14        0.989654       0.988640   21.971160      0m 21.97s       Yes   \n",
      "15        0.997875       0.992796   63.832153       1m 3.83s       Yes   \n",
      "16        0.995843       0.991965   68.736707       1m 8.74s       Yes   \n",
      "17        0.999630       0.992242   61.600496       1m 1.60s       Yes   \n",
      "18        0.990115       0.986700   38.629715      0m 38.63s       Yes   \n",
      "19        0.999261       0.990856  155.802536      2m 35.80s       Yes   \n",
      "20        0.989931       0.987531   37.823917      0m 37.82s       Yes   \n",
      "21        0.998430       0.987531  142.965929      2m 22.97s       Yes   \n",
      "22        0.998614       0.991133  176.060876      2m 56.06s       Yes   \n",
      "23        0.998430       0.991965  138.215482      2m 18.22s       Yes   \n",
      "24        0.989746       0.985869   53.542015      0m 53.54s       Yes   \n",
      "25        0.997875       0.987531  169.872821      2m 49.87s       Yes   \n",
      "\n",
      "              timestamp  \n",
      "0   2024-12-06 10:50:11  \n",
      "1   2024-12-06 10:50:27  \n",
      "2   2024-12-06 10:50:31  \n",
      "3   2024-12-06 10:50:36  \n",
      "4   2024-12-06 10:50:39  \n",
      "5   2024-12-06 10:50:41  \n",
      "6   2024-12-06 10:50:48  \n",
      "7   2024-12-06 10:50:51  \n",
      "8   2024-12-06 10:50:56  \n",
      "9   2024-12-06 10:51:11  \n",
      "10  2024-12-06 10:51:16  \n",
      "11  2024-12-06 10:51:37  \n",
      "12  2024-12-06 10:51:42  \n",
      "13  2024-12-06 10:51:45  \n",
      "14  2024-12-06 10:52:07  \n",
      "15  2024-12-06 10:53:11  \n",
      "16  2024-12-06 10:54:20  \n",
      "17  2024-12-06 10:55:22  \n",
      "18  2024-12-06 10:56:00  \n",
      "19  2024-12-06 10:58:36  \n",
      "20  2024-12-06 10:59:14  \n",
      "21  2024-12-06 11:01:37  \n",
      "22  2024-12-06 11:04:33  \n",
      "23  2024-12-06 11:06:52  \n",
      "24  2024-12-06 11:07:45  \n",
      "25  2024-12-06 11:10:36  \n"
     ]
    }
   ],
   "source": [
    "## Question: How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task \n",
    "# (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "digit0='3'\n",
    "digit1='7'\n",
    "mnist_bin_data=mnist.data[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "mnist_bin_target=mnist.target[np.logical_or(mnist.target==digit0,mnist.target==digit1)]\n",
    "print(\"The first datapoint now is: \\n\")\n",
    "plt.imshow(mnist_bin_data[0].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()\n",
    "print(mnist_bin_target)\n",
    "\n",
    "## Split the mnist_bin data into training and test set. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_bin_data, mnist_bin_target, random_state=1) # test size 25% and training 75%\n",
    "\n",
    "kernel_type = 'linear'\n",
    "start=time.time()\n",
    "linear_svm = SVC(kernel=kernel_type).fit(X_train,y_train)\n",
    "end=time.time()\n",
    "elapsed_time=end-start \n",
    "print(f\"Time it took to learn the {kernel_type} kernel model: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "train_accuracy = linear_svm.score(X_train, y_train)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "test_accuracy = linear_svm.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#print(\"Imports done and real code execution is starting at this time: \", time.strftime(\"%H:%M:%S\", time.localtime(time.time())))\n",
    "\n",
    "# Define architectures and activation functions to test\n",
    "architectures = [\n",
    "    (),                     # No hidden layers\n",
    "    (5,),                  # 1 layer, 5 neurons\n",
    "    (10,),                 # 1 layer, 10 neurons\n",
    "    (15,),                  ## one layer 15 neurons\n",
    "    (20,),\n",
    "    (5, 5),               # 2 layers, 5 neurons each\n",
    "    (10, 5),              # 2 layers, 10 neurons in first, 5 in second\n",
    "    (5, 5, 5),\n",
    "    (10, 10, 5),         # 3 layers\n",
    "    (10, 10, 10),\n",
    "    (15, 15, 15),\n",
    "    (5, 5, 5, 5),\n",
    "    (10, 10, 10, 10),\n",
    "]\n",
    "\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "# Configuration parameters\n",
    "solver = 'lbfgs'           # Solver type\n",
    "max_iter = 5000             # Number of iterations\n",
    "random_state_no = 11          # Random state for reproducibility\n",
    "\n",
    "file_path = \"mlp_results.csv\"\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for activation in activations:\n",
    "    for arch in architectures:\n",
    "        print(f\"Testing architecture: {arch} with activation: {activation}\")\n",
    "        start_model_and_train = time.time()\n",
    "        print(\"Start time is: \", time.strftime(\"%H:%M:%S\", time.localtime(start_model_and_train)))\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=arch, activation=activation, solver=solver, random_state=random_state_no, max_iter=max_iter)            # Number of iterations\n",
    "\n",
    "        # Train the model and capture warnings\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "            mlp.fit(X_train, y_train)\n",
    "            # Check for ConvergenceWarning\n",
    "            convergence_warning = any(issubclass(warn.category, ConvergenceWarning) for warn in w)\n",
    "\n",
    "        #mlp.fit(X_train, y_train)\n",
    "\n",
    "        end_model_and_train = time.time()\n",
    "        print(\"End time for model training is: \", time.strftime(\"%H:%M:%S\", time.localtime(time.time())))\n",
    "        \n",
    "        # Evaluate\n",
    "        train_acc = mlp.score(X_train, y_train)\n",
    "        test_acc = mlp.score(X_test, y_test)\n",
    "        elapsed_time = end_model_and_train - start_model_and_train\n",
    "\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        formatted_time = f\"{int(minutes)}m {seconds:.2f}s\"\n",
    "\n",
    "       \n",
    "        # Save results\n",
    "        results.append({\n",
    "            \"architecture\": arch,\n",
    "            \"activation\": activation,\n",
    "            \"solver\": solver,\n",
    "            \"iterations\": max_iter,\n",
    "            \"random_state\": random_state_no,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"time\": elapsed_time,\n",
    "            \"formatted_time\": formatted_time,\n",
    "            \"converged\": \"No (Did not converge)\" if convergence_warning else \"Yes\",\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        #print(f\"Activation: {activation}, Train Accuracy: {train_acc:.2f}, Test Accuracy: {test_acc:.2f}, Time: {time.strftime(\"%H:%M:%S\", time.localtime(elapsed_time))} seconds\\n\")\n",
    "        print(\"Before I move on time is: \", time.strftime(\"%H:%M:%S\", time.localtime(time.time())))\n",
    "\n",
    "# Create a DataFrame to save results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Append results to CSV\n",
    "if os.path.exists(file_path):\n",
    "    # Load existing data and append new results\n",
    "    existing_data = pd.read_csv(file_path)\n",
    "    updated_data = pd.concat([existing_data, results_df], ignore_index=True)\n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "else:\n",
    "    # Create a new CSV file\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Results saved to\", file_path)\n",
    "\n",
    "print(\"The results: \",results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prettytable in c:\\users\\kennethelong(keel)\\ideaprojects\\firstpythonproject\\firstpyhtonproject\\.venv\\lib\\site-packages (3.12.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kennethelong(keel)\\ideaprojects\\firstpythonproject\\firstpyhtonproject\\.venv\\lib\\site-packages (from prettytable) (0.2.13)\n",
      "Processing pair: 7 and 3\n",
      "Training SVC for pair 7-3\n",
      "Training NN for pair 7-3 with architecture ()\n",
      "Training NN for pair 7-3 with architecture (20,)\n",
      "Training NN for pair 7-3 with architecture (5, 5, 5)\n",
      "Processing pair: 4 and 5\n",
      "Training SVC for pair 4-5\n",
      "Training NN for pair 4-5 with architecture ()\n",
      "Training NN for pair 4-5 with architecture (20,)\n",
      "Training NN for pair 4-5 with architecture (5, 5, 5)\n",
      "Processing pair: 0 and 1\n",
      "Training SVC for pair 0-1\n",
      "Training NN for pair 0-1 with architecture ()\n",
      "Training NN for pair 0-1 with architecture (20,)\n",
      "Training NN for pair 0-1 with architecture (5, 5, 5)\n",
      "\n",
      "Results Summary:\n",
      "\n",
      "+------+-------+--------------+--------+------------+--------------------+-------------------+----------------+-----------+---------------------+\n",
      "| Pair | Model | Architecture | Solver | Iterations | Train Accuracy (%) | Test Accuracy (%) | Time (min:sec) | Converged |      Timestamp      |\n",
      "+------+-------+--------------+--------+------------+--------------------+-------------------+----------------+-----------+---------------------+\n",
      "| 7-3  |  SVC  |     N/A      | linear |    N/A     |      100.0000      |      98.1158      |     0:3.25     |    Yes    | 2024-12-10 08:24:34 |\n",
      "| 7-3  |   NN  |      ()      | lbfgs  |    5000    |      100.0000      |      98.0327      |     0:2.36     |    Yes    | 2024-12-10 08:24:36 |\n",
      "| 7-3  |   NN  |    (20,)     | lbfgs  |    5000    |      99.9908       |      99.3904      |     0:3.54     |    Yes    | 2024-12-10 08:24:40 |\n",
      "| 7-3  |   NN  |  (5, 5, 5)   | lbfgs  |    5000    |      99.8430       |      98.5314      |     0:7.61     |    Yes    | 2024-12-10 08:24:48 |\n",
      "| 4-5  |  SVC  |     N/A      | linear |    N/A     |      100.0000      |      98.4779      |     0:2.38     |    Yes    | 2024-12-10 08:24:51 |\n",
      "| 4-5  |   NN  |      ()      | lbfgs  |    5000    |      100.0000      |      98.6301      |     0:1.32     |    Yes    | 2024-12-10 08:24:52 |\n",
      "| 4-5  |   NN  |    (20,)     | lbfgs  |    5000    |      100.0000      |      99.6043      |     0:1.58     |    Yes    | 2024-12-10 08:24:54 |\n",
      "| 4-5  |   NN  |  (5, 5, 5)   | lbfgs  |    5000    |      99.9594       |      99.4825      |     0:4.26     |    Yes    | 2024-12-10 08:24:58 |\n",
      "| 0-1  |  SVC  |     N/A      | linear |    N/A     |      100.0000      |      99.7294      |     0:0.63     |    Yes    | 2024-12-10 08:24:59 |\n",
      "| 0-1  |   NN  |      ()      | lbfgs  |    5000    |      100.0000      |      99.7294      |     0:0.24     |    Yes    | 2024-12-10 08:25:00 |\n",
      "| 0-1  |   NN  |    (20,)     | lbfgs  |    5000    |      100.0000      |      99.8376      |     0:1.42     |    Yes    | 2024-12-10 08:25:01 |\n",
      "| 0-1  |   NN  |  (5, 5, 5)   | lbfgs  |    5000    |      100.0000      |      99.8376      |     0:5.37     |    Yes    | 2024-12-10 08:25:07 |\n",
      "+------+-------+--------------+--------+------------+--------------------+-------------------+----------------+-----------+---------------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>model</th>\n",
       "      <th>architecture</th>\n",
       "      <th>solver</th>\n",
       "      <th>iterations</th>\n",
       "      <th>train_accuracy (%)</th>\n",
       "      <th>test_accuracy (%)</th>\n",
       "      <th>time (min:sec)</th>\n",
       "      <th>converged</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7-3</td>\n",
       "      <td>SVC</td>\n",
       "      <td>N/A</td>\n",
       "      <td>linear</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>98.1158</td>\n",
       "      <td>0:3.25</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-3</td>\n",
       "      <td>NN</td>\n",
       "      <td>()</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>98.0327</td>\n",
       "      <td>0:2.36</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-3</td>\n",
       "      <td>NN</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>99.9908</td>\n",
       "      <td>99.3904</td>\n",
       "      <td>0:3.54</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7-3</td>\n",
       "      <td>NN</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>99.8430</td>\n",
       "      <td>98.5314</td>\n",
       "      <td>0:7.61</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4-5</td>\n",
       "      <td>SVC</td>\n",
       "      <td>N/A</td>\n",
       "      <td>linear</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>98.4779</td>\n",
       "      <td>0:2.38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4-5</td>\n",
       "      <td>NN</td>\n",
       "      <td>()</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>98.6301</td>\n",
       "      <td>0:1.32</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4-5</td>\n",
       "      <td>NN</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>99.6043</td>\n",
       "      <td>0:1.58</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4-5</td>\n",
       "      <td>NN</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>99.9594</td>\n",
       "      <td>99.4825</td>\n",
       "      <td>0:4.26</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0-1</td>\n",
       "      <td>SVC</td>\n",
       "      <td>N/A</td>\n",
       "      <td>linear</td>\n",
       "      <td>N/A</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>99.7294</td>\n",
       "      <td>0:0.63</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:24:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0-1</td>\n",
       "      <td>NN</td>\n",
       "      <td>()</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>99.7294</td>\n",
       "      <td>0:0.24</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0-1</td>\n",
       "      <td>NN</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>99.8376</td>\n",
       "      <td>0:1.42</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:25:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0-1</td>\n",
       "      <td>NN</td>\n",
       "      <td>(5, 5, 5)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>5000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>99.8376</td>\n",
       "      <td>0:5.37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-12-10 08:25:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair model architecture  solver iterations train_accuracy (%)  \\\n",
       "0   7-3   SVC          N/A  linear        N/A           100.0000   \n",
       "1   7-3    NN           ()   lbfgs       5000           100.0000   \n",
       "2   7-3    NN        (20,)   lbfgs       5000            99.9908   \n",
       "3   7-3    NN    (5, 5, 5)   lbfgs       5000            99.8430   \n",
       "4   4-5   SVC          N/A  linear        N/A           100.0000   \n",
       "5   4-5    NN           ()   lbfgs       5000           100.0000   \n",
       "6   4-5    NN        (20,)   lbfgs       5000           100.0000   \n",
       "7   4-5    NN    (5, 5, 5)   lbfgs       5000            99.9594   \n",
       "8   0-1   SVC          N/A  linear        N/A           100.0000   \n",
       "9   0-1    NN           ()   lbfgs       5000           100.0000   \n",
       "10  0-1    NN        (20,)   lbfgs       5000           100.0000   \n",
       "11  0-1    NN    (5, 5, 5)   lbfgs       5000           100.0000   \n",
       "\n",
       "   test_accuracy (%) time (min:sec) converged            timestamp  \n",
       "0            98.1158         0:3.25       Yes  2024-12-10 08:24:34  \n",
       "1            98.0327         0:2.36       Yes  2024-12-10 08:24:36  \n",
       "2            99.3904         0:3.54       Yes  2024-12-10 08:24:40  \n",
       "3            98.5314         0:7.61       Yes  2024-12-10 08:24:48  \n",
       "4            98.4779         0:2.38       Yes  2024-12-10 08:24:51  \n",
       "5            98.6301         0:1.32       Yes  2024-12-10 08:24:52  \n",
       "6            99.6043         0:1.58       Yes  2024-12-10 08:24:54  \n",
       "7            99.4825         0:4.26       Yes  2024-12-10 08:24:58  \n",
       "8            99.7294         0:0.63       Yes  2024-12-10 08:24:59  \n",
       "9            99.7294         0:0.24       Yes  2024-12-10 08:25:00  \n",
       "10           99.8376         0:1.42       Yes  2024-12-10 08:25:01  \n",
       "11           99.8376         0:5.37       Yes  2024-12-10 08:25:07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results also saved to pair_runs.csv\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "## Question: How do results (time and accuracy) change, depending on whether you consider an 'easy' binary task \n",
    "# (e.g., distinguishing '1' and '0'), or a more difficult one (e.g., '4' vs. '5'). \n",
    "\n",
    "## This next code does a couple of things, let's run through the idea\n",
    "## In the previous section a number of models were identified that had a good performance, in that accuracy on the test set was very good and the time to learn the model was also better than its peers. These \n",
    "#  were based on comparing 3 and 7\n",
    "## The purpose of this section is to try a number of pairs to see if performance for the models that performed well on 3 and 7 is also good on these pairs. \n",
    "\n",
    "## Specifically the pairs 3 and 7, 4 and 5, as well as 0 and 1 are tried. The architecture is configured in the beginning, using the same resolver and with a preset number of max iterations. During the experiment I did change\n",
    "#  the number of iterations.\n",
    "## The results along the configuration is saved in a csv file.\n",
    "\n",
    "# Define digit pairs, neural network configurations, and result storage\n",
    "digit_pairs = [(7, 3), (4, 5), (0, 1)]  # Pairs to test\n",
    "nn_configurations = [\n",
    "    {\"architecture\": (), \"solver\": \"lbfgs\", \"iterations\": 5000},\n",
    "    {\"architecture\": (20,), \"solver\": \"lbfgs\", \"iterations\": 5000},\n",
    "    {\"architecture\": (5, 5, 5), \"solver\": \"lbfgs\", \"iterations\": 5000},\n",
    "]\n",
    "results = []  # List to store results\n",
    "file_path = \"pair_runs.csv\"  # CSV file for results\n",
    "\n",
    "# Loop through each digit pair\n",
    "for digit1, digit2 in digit_pairs:\n",
    "    print(f\"Processing pair: {digit1} and {digit2}\")\n",
    "\n",
    "    # Filter dataset for the current pair\n",
    "    indices = np.logical_or(mnist.target == str(digit1), mnist.target == str(digit2))\n",
    "    X_pair = mnist.data[indices]\n",
    "    y_pair = mnist.target[indices]\n",
    "    y_pair = np.where(y_pair == str(digit1), digit1, digit2)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pair, y_pair, random_state=11)\n",
    "\n",
    "    # Train SVC model\n",
    "    print(f\"Training SVC for pair {digit1}-{digit2}\")\n",
    "    start_svc = time.time()\n",
    "    svc = SVC(kernel=\"linear\", random_state=11)\n",
    "    svc.fit(X_train, y_train)\n",
    "    end_svc = time.time()\n",
    "\n",
    "    # Evaluate SVC model\n",
    "    svc_train_acc = round(svc.score(X_train, y_train) * 100, 4)\n",
    "    svc_test_acc = round(svc.score(X_test, y_test) * 100, 4)\n",
    "    svc_time_minutes, svc_time_seconds = divmod(end_svc - start_svc, 60)\n",
    "\n",
    "    # Save SVC results\n",
    "    results.append({\n",
    "        \"pair\": f\"{digit1}-{digit2}\",\n",
    "        \"model\": \"SVC\",\n",
    "        \"architecture\": \"N/A\",\n",
    "        \"solver\": \"linear\",\n",
    "        \"iterations\": \"N/A\",\n",
    "        \"train_accuracy (%)\": f\"{svc_train_acc:.4f}\",\n",
    "        \"test_accuracy (%)\": f\"{svc_test_acc:.4f}\",\n",
    "        \"time (min:sec)\": f\"{int(svc_time_minutes)}:{svc_time_seconds:.2f}\",\n",
    "        \"converged\": \"Yes\",\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    })\n",
    "\n",
    "    # Train NN models\n",
    "    for config in nn_configurations:\n",
    "        arch = config[\"architecture\"]\n",
    "        solver = config[\"solver\"]\n",
    "        iterations = config[\"iterations\"]\n",
    "\n",
    "        print(f\"Training NN for pair {digit1}-{digit2} with architecture {arch}\")\n",
    "        start_nn = time.time()\n",
    "        nn = MLPClassifier(hidden_layer_sizes=arch, solver=solver, max_iter=iterations, random_state=11)\n",
    "        \n",
    "        try:\n",
    "            nn.fit(X_train, y_train)\n",
    "            converged = \"Yes\"\n",
    "        except Exception as e:\n",
    "            converged = \"No\"\n",
    "\n",
    "        end_nn = time.time()\n",
    "\n",
    "        # Evaluate NN model\n",
    "        if converged == \"Yes\":\n",
    "            nn_train_acc = round(nn.score(X_train, y_train) * 100, 4)\n",
    "            nn_test_acc = round(nn.score(X_test, y_test) * 100, 4)\n",
    "        else:\n",
    "            nn_train_acc = nn_test_acc = \"N/A\"\n",
    "\n",
    "        nn_time_minutes, nn_time_seconds = divmod(end_nn - start_nn, 60)\n",
    "\n",
    "        # Save NN results\n",
    "        results.append({\n",
    "            \"pair\": f\"{digit1}-{digit2}\",\n",
    "            \"model\": \"NN\",\n",
    "            \"architecture\": f\"{arch}\",\n",
    "            \"solver\": solver,\n",
    "            \"iterations\": iterations,\n",
    "            \"train_accuracy (%)\": f\"{nn_train_acc:.4f}\" if nn_train_acc != \"N/A\" else \"N/A\",\n",
    "            \"test_accuracy (%)\": f\"{nn_test_acc:.4f}\" if nn_test_acc != \"N/A\" else \"N/A\",\n",
    "            \"time (min:sec)\": f\"{int(nn_time_minutes)}:{nn_time_seconds:.2f}\",\n",
    "            \"converged\": converged,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "if os.path.exists(file_path):\n",
    "    existing_data = pd.read_csv(file_path)\n",
    "    updated_data = pd.concat([existing_data, results_df], ignore_index=True)\n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "else:\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Display results in the terminal as a table using PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\n",
    "    \"Pair\", \"Model\", \"Architecture\", \"Solver\", \"Iterations\",\n",
    "    \"Train Accuracy (%)\", \"Test Accuracy (%)\", \"Time (min:sec)\", \"Converged\", \"Timestamp\"\n",
    "]\n",
    "\n",
    "for result in results:\n",
    "    table.add_row([\n",
    "        result[\"pair\"],\n",
    "        result[\"model\"],\n",
    "        result[\"architecture\"],\n",
    "        result[\"solver\"],\n",
    "        result[\"iterations\"],\n",
    "        result[\"train_accuracy (%)\"],\n",
    "        result[\"test_accuracy (%)\"],\n",
    "        result[\"time (min:sec)\"],\n",
    "        result[\"converged\"],\n",
    "        result[\"timestamp\"],\n",
    "    ])\n",
    "\n",
    "print(\"\\nResults Summary:\\n\")\n",
    "print(table)\n",
    "\n",
    "# Display results as a pandas table in Jupyter\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nResults also saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the SVC and three different architectures of the NN that overall most precision were obtained in the 0-1 pair. All test accuracy is above 99.7%. The 7-3 pair was the overall lowest with the lowest being 98% and the highest being 99.3%.\n",
    "\n",
    "The time was between 5 and 0 seconds. For e.g. pair 0-1 the NN with 20 nodes had the same performance as the three layered with 5 nodes at 99.8% but the three layered architecture took 5:37 seconds and the 1 layered only 1:42 seconds around 3 times as long. The architecture without any hidden layer was very fast for 0-1 at 0:24 seconds and had a very good accuracy at 99./\n",
    "\n",
    "Generally it can be said that accuracy and time taking into account one layer with 20 nodes performed really well balancing time and accuracy. The 3 layered most often took longer than the rest. The perceptron and the SVC were contending for second best performance and only had slight performance differences on this smaller data set. Let's try them both in the next section to see if a more clear performance emerges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC on full dataset\n"
     ]
    }
   ],
   "source": [
    "## Exercise: Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full \n",
    "# classification of the 10 classes in the original dataset (after split into train/test).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time\n",
    "from datetime import datetime\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Load the full MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.data = mnist.data.astype(np.float32)\n",
    "mnist.target = mnist.target.astype(str)\n",
    "\n",
    "# Neural network configurations to run\n",
    "nn_configurations = [\n",
    "    {\"architecture\": (), \"solver\": \"lbfgs\", \"iterations\": 5000},\n",
    "    {\"architecture\": (20,), \"solver\": \"lbfgs\", \"iterations\": 5000},\n",
    "]\n",
    "\n",
    "results = []  # List to store results\n",
    "file_path = \"full_dataset_runs.csv\"  # Output file\n",
    "\n",
    "# Split into train and test sets for the entire dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.2, random_state=11)\n",
    "\n",
    "# Train SVC model\n",
    "print(f\"Training SVC on full dataset\")\n",
    "start_svc = time.time()\n",
    "svc = SVC(kernel=\"linear\", random_state=11)\n",
    "svc.fit(X_train, y_train)\n",
    "end_svc = time.time()\n",
    "\n",
    "# Evaluate SVC model\n",
    "svc_train_acc = round(svc.score(X_train, y_train) * 100, 4)\n",
    "svc_test_acc = round(svc.score(X_test, y_test) * 100, 4)\n",
    "svc_time_minutes, svc_time_seconds = divmod(end_svc - start_svc, 60)\n",
    "\n",
    "# Save SVC results\n",
    "results.append({\n",
    "    \"model\": \"SVC\",\n",
    "    \"architecture\": \"N/A\",\n",
    "    \"solver\": \"linear\",\n",
    "    \"iterations\": \"N/A\",\n",
    "    \"train_accuracy (%)\": f\"{svc_train_acc:.4f}\",\n",
    "    \"test_accuracy (%)\": f\"{svc_test_acc:.4f}\",\n",
    "    \"time (min:sec)\": f\"{int(svc_time_minutes)}:{svc_time_seconds:.2f}\",\n",
    "    \"converged\": \"Yes\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "})\n",
    "\n",
    "# Train NN models\n",
    "for config in nn_configurations:\n",
    "    arch = config[\"architecture\"]\n",
    "    solver = config[\"solver\"]\n",
    "    iterations = config[\"iterations\"]\n",
    "\n",
    "    print(f\"Training NN with architecture {arch}\")\n",
    "    start_nn = time.time()\n",
    "    nn = MLPClassifier(hidden_layer_sizes=arch, solver=solver, max_iter=iterations, random_state=11)\n",
    "    \n",
    "    try:\n",
    "        nn.fit(X_train, y_train)\n",
    "        converged = \"Yes\"\n",
    "    except Exception as e:\n",
    "        converged = \"No\"\n",
    "\n",
    "    end_nn = time.time()\n",
    "\n",
    "    # Evaluate NN model\n",
    "    if converged == \"Yes\":\n",
    "        nn_train_acc = round(nn.score(X_train, y_train) * 100, 4)\n",
    "        nn_test_acc = round(nn.score(X_test, y_test) * 100, 4)\n",
    "    else:\n",
    "        nn_train_acc = nn_test_acc = \"N/A\"\n",
    "\n",
    "    nn_time_minutes, nn_time_seconds = divmod(end_nn - start_nn, 60)\n",
    "\n",
    "    # Save NN results\n",
    "    results.append({\n",
    "        \"model\": \"NN\",\n",
    "        \"architecture\": f\"{arch}\",\n",
    "        \"solver\": solver,\n",
    "        \"iterations\": iterations,\n",
    "        \"train_accuracy (%)\": f\"{nn_train_acc:.4f}\" if nn_train_acc != \"N/A\" else \"N/A\",\n",
    "        \"test_accuracy (%)\": f\"{nn_test_acc:.4f}\" if nn_test_acc != \"N/A\" else \"N/A\",\n",
    "        \"time (min:sec)\": f\"{int(nn_time_minutes)}:{nn_time_seconds:.2f}\",\n",
    "        \"converged\": converged,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    })\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Display results as a PrettyTable\n",
    "table = PrettyTable()\n",
    "table.field_names = [\n",
    "    \"Model\", \"Architecture\", \"Solver\", \"Iterations\",\n",
    "    \"Train Accuracy (%)\", \"Test Accuracy (%)\", \"Time (min:sec)\", \"Converged\", \"Timestamp\"\n",
    "]\n",
    "\n",
    "for result in results:\n",
    "    table.add_row([\n",
    "        result[\"model\"],\n",
    "        result[\"architecture\"],\n",
    "        result[\"solver\"],\n",
    "        result[\"iterations\"],\n",
    "        result[\"train_accuracy (%)\"],\n",
    "        result[\"test_accuracy (%)\"],\n",
    "        result[\"time (min:sec)\"],\n",
    "        result[\"converged\"],\n",
    "        result[\"timestamp\"],\n",
    "    ])\n",
    "\n",
    "print(\"\\nResults Summary:\\n\")\n",
    "print(table)\n",
    "\n",
    "# Display results inline in Jupyter\n",
    "from IPython.display import display\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nResults saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC on full dataset\n"
     ]
    }
   ],
   "source": [
    "## ----------------- OBSOLETE TO BE DELETED\n",
    "\n",
    "## Exercise: Identify one or several good configurations that give a reasonable combination of accuracy and runtime. Use these configurations to perform a full \n",
    "# classification of the 10 classes in the original dataset (after split into train/test).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)  # Ensure labels are integers\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# File path for saving results\n",
    "file_path = \"full_dataset_runs.csv\"\n",
    "\n",
    "def format_time(seconds):\n",
    "    # Convert time in seconds to hh:mm:ss format.\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{seconds:05.2f}\"\n",
    "\n",
    "### Train SVC on full dataset ###\n",
    "print(\"Training SVC on full dataset\")\n",
    "start_svc = time.time()\n",
    "svc = SVC(kernel=\"linear\", random_state=11)\n",
    "svc.fit(X_train, y_train)\n",
    "end_svc = time.time()\n",
    "\n",
    "# Evaluate SVC\n",
    "svc_train_acc = svc.score(X_train, y_train)\n",
    "svc_test_acc = svc.score(X_test, y_test)\n",
    "svc_time = end_svc - start_svc\n",
    "\n",
    "# Save SVC results\n",
    "results.append({\n",
    "    \"model\": \"svc_full\",\n",
    "    \"train_accuracy\": svc_train_acc,\n",
    "    \"test_accuracy\": svc_test_acc,\n",
    "    \"time\": format_time(svc_time),\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "})\n",
    "\n",
    "### Train NN on full dataset ###\n",
    "print(\"Training NN on full dataset with 1 layer (20 nodes)\")\n",
    "start_nn = time.time()\n",
    "nn = MLPClassifier(hidden_layer_sizes=(20,), solver=\"lbfgs\", max_iter=500, random_state=11)\n",
    "\n",
    "try:\n",
    "    nn.fit(X_train, y_train)\n",
    "    converged = \"Yes\"\n",
    "except Exception as e:\n",
    "    converged = \"No\"\n",
    "end_nn = time.time()\n",
    "\n",
    "# Evaluate NN\n",
    "nn_train_acc = nn.score(X_train, y_train) if converged == \"Yes\" else \"N/A\"\n",
    "nn_test_acc = nn.score(X_test, y_test) if converged == \"Yes\" else \"N/A\"\n",
    "nn_time = end_nn - start_nn\n",
    "\n",
    "# Save NN results\n",
    "results.append({\n",
    "    \"model\": \"nn_full_20\",\n",
    "    \"architecture\": (20,),\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"iterations\": 500,\n",
    "    \"train_accuracy\": nn_train_acc,\n",
    "    \"test_accuracy\": nn_test_acc,\n",
    "    \"time\": format_time(nn_time),\n",
    "    \"converged\": converged,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "})\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Append new data to the existing file\n",
    "    existing_data = pd.read_csv(file_path)\n",
    "    updated_data = pd.concat([existing_data, results_df], ignore_index=True)\n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "else:\n",
    "    # Create a new CSV file\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads the data and makes a train and a test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data fetched\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "    \n",
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "print(\"data fetched\")\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)  # Ensure labels are integers\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code iterates through a number of architectures with the full data set and a set of max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2024-12-06 09:40:06 with layers=(), solver=lbfgs, max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed at 2024-12-06 09:41:13\n",
      "Evaluation started at 2024-12-06 09:41:13\n",
      "Evaluation completed at 2024-12-06 09:41:13\n",
      "Iteration Result: {'layers': (), 'solver': 'lbfgs', 'max_iter': 500, 'train_accuracy (%)': '92.9107', 'test_accuracy (%)': '89.8786', 'training_time': '00:01:07', 'evaluation_time': '00:00:00', 'converged': 'No (Reached max_iter)', 'start_time': '2024-12-06 09:40:06', 'end_time': '2024-12-06 09:41:13'}\n",
      "Training started at 2024-12-06 09:41:13 with layers=(), solver=lbfgs, max_iter=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed at 2024-12-06 09:45:37\n",
      "Evaluation started at 2024-12-06 09:45:37\n",
      "Evaluation completed at 2024-12-06 09:45:37\n",
      "Iteration Result: {'layers': (), 'solver': 'lbfgs', 'max_iter': 2000, 'train_accuracy (%)': '94.3268', 'test_accuracy (%)': '91.4071', 'training_time': '00:04:23', 'evaluation_time': '00:00:00', 'converged': 'No (Reached max_iter)', 'start_time': '2024-12-06 09:41:13', 'end_time': '2024-12-06 09:45:37'}\n",
      "Training started at 2024-12-06 09:45:37 with layers=(), solver=lbfgs, max_iter=5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed at 2024-12-06 09:56:09\n",
      "Evaluation started at 2024-12-06 09:56:09\n",
      "Evaluation completed at 2024-12-06 09:56:09\n",
      "Iteration Result: {'layers': (), 'solver': 'lbfgs', 'max_iter': 5000, 'train_accuracy (%)': '94.5232', 'test_accuracy (%)': '91.4857', 'training_time': '00:10:31', 'evaluation_time': '00:00:00', 'converged': 'No (Reached max_iter)', 'start_time': '2024-12-06 09:45:37', 'end_time': '2024-12-06 09:56:09'}\n",
      "Training started at 2024-12-06 09:56:09 with layers=(5,), solver=lbfgs, max_iter=500\n",
      "Training completed at 2024-12-06 09:56:11\n",
      "Evaluation started at 2024-12-06 09:56:11\n",
      "Evaluation completed at 2024-12-06 09:56:11\n",
      "Iteration Result: {'layers': (5,), 'solver': 'lbfgs', 'max_iter': 500, 'train_accuracy (%)': '11.2089', 'test_accuracy (%)': '11.4286', 'training_time': '00:00:02', 'evaluation_time': '00:00:00', 'converged': 'Yes', 'start_time': '2024-12-06 09:56:09', 'end_time': '2024-12-06 09:56:11'}\n",
      "Training started at 2024-12-06 09:56:11 with layers=(5,), solver=lbfgs, max_iter=2000\n",
      "Training completed at 2024-12-06 09:56:13\n",
      "Evaluation started at 2024-12-06 09:56:13\n",
      "Evaluation completed at 2024-12-06 09:56:13\n",
      "Iteration Result: {'layers': (5,), 'solver': 'lbfgs', 'max_iter': 2000, 'train_accuracy (%)': '11.2089', 'test_accuracy (%)': '11.4286', 'training_time': '00:00:02', 'evaluation_time': '00:00:00', 'converged': 'Yes', 'start_time': '2024-12-06 09:56:11', 'end_time': '2024-12-06 09:56:13'}\n",
      "Training started at 2024-12-06 09:56:13 with layers=(5,), solver=lbfgs, max_iter=5000\n",
      "Training completed at 2024-12-06 09:56:15\n",
      "Evaluation started at 2024-12-06 09:56:15\n",
      "Evaluation completed at 2024-12-06 09:56:15\n",
      "Iteration Result: {'layers': (5,), 'solver': 'lbfgs', 'max_iter': 5000, 'train_accuracy (%)': '11.2089', 'test_accuracy (%)': '11.4286', 'training_time': '00:00:02', 'evaluation_time': '00:00:00', 'converged': 'Yes', 'start_time': '2024-12-06 09:56:13', 'end_time': '2024-12-06 09:56:15'}\n",
      "Training started at 2024-12-06 09:56:15 with layers=(15,), solver=lbfgs, max_iter=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed at 2024-12-06 09:57:44\n",
      "Evaluation started at 2024-12-06 09:57:44\n",
      "Evaluation completed at 2024-12-06 09:57:44\n",
      "Iteration Result: {'layers': (15,), 'solver': 'lbfgs', 'max_iter': 500, 'train_accuracy (%)': '51.4643', 'test_accuracy (%)': '51.4357', 'training_time': '00:01:28', 'evaluation_time': '00:00:00', 'converged': 'No (Reached max_iter)', 'start_time': '2024-12-06 09:56:15', 'end_time': '2024-12-06 09:57:44'}\n",
      "Training started at 2024-12-06 09:57:44 with layers=(15,), solver=lbfgs, max_iter=2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mtrain_and_evaluate_nn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterations_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 51\u001b[0m, in \u001b[0;36mtrain_and_evaluate_nn\u001b[1;34m(layers_list, solver, iterations_list, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_iter:\n\u001b[0;32m     53\u001b[0m         converged \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo (Reached max_iter)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:751\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:488\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_units\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# validate parameter weights\u001b[39;00m\n\u001b[0;32m    493\u001b[0m weights \u001b[38;5;241m=\u001b[39m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefs_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_)\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:532\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 532\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_grad_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_coef_inter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:343\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:281\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[1;32m--> 281\u001b[0m loss, coef_grads, intercept_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m grad \u001b[38;5;241m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:326\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    323\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Forward propagate\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Get loss\u001b[39;00m\n\u001b[0;32m    329\u001b[0m loss_func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:173\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass\u001b[1;34m(self, activations)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Iterate over the hidden layers\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     activations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefs_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     activations[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_[i]\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# For the hidden layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:208\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 208\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1335\u001b[0m, in \u001b[0;36missparse\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A namespace class to separate sparray from spmatrix\"\"\"\u001b[39;00m\n\u001b[0;32m   1332\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[1;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# File path for saving results\n",
    "file_path = \"full_dataset_results.csv\"\n",
    "\n",
    "# Function to format timestamps (start and end)\n",
    "def format_timestamp(seconds=None):\n",
    "    \"\"\"Format epoch time to YYYY-MM-DD HH:MM:SS.\"\"\"\n",
    "    if seconds is None:\n",
    "        seconds = time.time()\n",
    "    return datetime.fromtimestamp(seconds).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Function to format durations in hh:mm:ss\n",
    "def format_duration(seconds):\n",
    "    \"\"\"Convert duration in seconds to hh:mm:ss format.\"\"\"\n",
    "    hours, remainder = divmod(int(seconds), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "# Function to append results to the CSV file\n",
    "def append_result_to_csv(result, file_path):\n",
    "    \"\"\"Append a single result row to the CSV file.\"\"\"\n",
    "    result_df = pd.DataFrame([result])\n",
    "    if not os.path.exists(file_path):\n",
    "        result_df.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        updated_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "        updated_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate_nn(layers_list, solver, iterations_list, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a neural network.\"\"\"\n",
    "    for layers in layers_list:\n",
    "        for max_iter in iterations_list:\n",
    "            try:\n",
    "                # Log start time\n",
    "                start_train = time.time()\n",
    "                start_timestamp = format_timestamp(start_train)\n",
    "                print(f\"Training started at {start_timestamp} with layers={layers}, solver={solver}, max_iter={max_iter}\")\n",
    "\n",
    "                # Initialize NN model\n",
    "                nn = MLPClassifier(hidden_layer_sizes=layers, solver=solver, max_iter=max_iter, random_state=11)\n",
    "\n",
    "                # Train model\n",
    "                try:\n",
    "                    nn.fit(X_train, y_train)\n",
    "                    if nn.n_iter_ >= max_iter:\n",
    "                        converged = \"No (Reached max_iter)\"\n",
    "                    else:\n",
    "                        converged = \"Yes\"\n",
    "                except Exception as train_error:\n",
    "                    converged = f\"No (Error: {train_error})\"\n",
    "                    print(f\"Training failed: {train_error}\")\n",
    "                    train_acc = \"N/A\"\n",
    "                    test_acc = \"N/A\"\n",
    "                else:\n",
    "                    train_acc = nn.score(X_train, y_train) * 100\n",
    "                    test_acc = nn.score(X_test, y_test) * 100\n",
    "\n",
    "                end_train = time.time()\n",
    "                end_timestamp = format_timestamp(end_train)\n",
    "                training_duration = format_duration(end_train - start_train)\n",
    "                print(f\"Training completed at {end_timestamp}\")\n",
    "\n",
    "                # Evaluate model\n",
    "                start_eval = time.time()\n",
    "                print(f\"Evaluation started at {format_timestamp(start_eval)}\")\n",
    "                if converged.startswith(\"Yes\"):\n",
    "                    evaluation_duration = \"00:00:00\"  # lbfgs solver evaluates implicitly during training\n",
    "                else:\n",
    "                    evaluation_duration = format_duration(time.time() - start_eval)\n",
    "\n",
    "                print(f\"Evaluation completed at {format_timestamp(time.time())}\")\n",
    "\n",
    "                # Record results\n",
    "                result = {\n",
    "                    \"layers\": layers,\n",
    "                    \"solver\": solver,\n",
    "                    \"max_iter\": max_iter,\n",
    "                    \"train_accuracy (%)\": f\"{train_acc:.4f}\" if train_acc != \"N/A\" else \"N/A\",\n",
    "                    \"test_accuracy (%)\": f\"{test_acc:.4f}\" if test_acc != \"N/A\" else \"N/A\",\n",
    "                    \"training_time\": training_duration,\n",
    "                    \"evaluation_time\": evaluation_duration,\n",
    "                    \"converged\": converged,\n",
    "                    \"start_time\": start_timestamp,\n",
    "                    \"end_time\": end_timestamp\n",
    "                }\n",
    "\n",
    "                # Print the result of the current iteration\n",
    "                print(f\"Iteration Result: {result}\")\n",
    "\n",
    "                # Append the result to the CSV\n",
    "                append_result_to_csv(result, file_path)\n",
    "\n",
    "            except Exception as error:\n",
    "                print(f\"An error occurred during training with layers={layers} and max_iter={max_iter}: {error}\")\n",
    "                continue  # Move on to the next iteration\n",
    "\n",
    "# Example Usage\n",
    "# Assume X_train, X_test, y_train, y_test are already loaded and split elsewhere\n",
    "# Define parameters\n",
    "layers_list = [\n",
    "    (),  # No hidden layers\n",
    "    (5,),  # 1 layer, 5 neurons\n",
    "    (15,),  # 1 layer, 15 neurons\n",
    "    (5, 5),  # 2 layers, 5 neurons each\n",
    "    (10, 5),  # 2 layers, 10 neurons in first, 5 in second\n",
    "    (15, 15),  # 2 layers, 15 neurons each\n",
    "    (5, 5, 5),  # 3 layers, 5 neurons each\n",
    "    # (10,),  # 1 layer, 10 neurons (Commented out)\n",
    "    # (20,),  # 1 layer, 20 neurons (Commented out)\n",
    "    # (10, 10, 5),  # 3 layers, 10 neurons each in the first two layers, 5 in the third\n",
    "    # (10, 10, 10),  # 3 layers, 10 neurons each\n",
    "    (15, 15, 15),  # 3 layers, 15 neurons each\n",
    "    # (5, 5, 5, 5),  # 4 layers, 5 neurons each (Commented out)\n",
    "    (10, 10, 10, 10),  # 4 layers, 10 neurons each\n",
    "]\n",
    "iterations_list = [500, 2000, 5000]\n",
    "solver = \"lbfgs\"\n",
    "\n",
    "# Call the function\n",
    "train_and_evaluate_nn(\n",
    "    layers_list=layers_list,\n",
    "    solver=solver,\n",
    "    iterations_list=iterations_list,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below build a NN model with 4 layers, 10 nodes and 5000 in max iterations and makes a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise: Using `sklearn.metrics.confusion_matrix` you can get an overview of all combinations of true and predicted labels (\n",
    "# see p. 298-299 in Müller & Guido). What does this tell you about which digits are easy, and which ones are difficult to recognize, and which ones are most easily confused?\n",
    "\n",
    "## Building a NN with 4 layers and 10 nodes, using relu and lbfgs and 5000 iterations\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the configuration\n",
    "layers = (10, 10, 10, 10)\n",
    "activation = \"relu\"\n",
    "solver = \"lbfgs\"\n",
    "max_iter = 5000\n",
    "\n",
    "def build_and_evaluate_nn(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=layers,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training the model with layers={layers}, activation={activation}, solver={solver}, max_iter={max_iter}\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        return\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix for Model Configuration: {layers}, {activation}, {solver}, {max_iter}\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assume X_train, X_test, y_train, and y_test are already defined.\n",
    "# Uncomment the line below to run the function with your dataset:\n",
    "# build_and_evaluate_nn(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Iteration 1, loss = 0.29130293\n",
      "Iteration 2, loss = 0.10562392\n",
      "Iteration 3, loss = 0.06646595\n",
      "Iteration 4, loss = 0.04825473\n",
      "Iteration 5, loss = 0.03408653\n",
      "Iteration 6, loss = 0.02480841\n",
      "Iteration 7, loss = 0.01772750\n",
      "Iteration 8, loss = 0.01742728\n",
      "Iteration 9, loss = 0.01654215\n",
      "Iteration 10, loss = 0.01820463\n",
      "Iteration 11, loss = 0.01834300\n",
      "Iteration 12, loss = 0.00950696\n",
      "Iteration 13, loss = 0.00726776\n",
      "Iteration 14, loss = 0.01606887\n",
      "Iteration 15, loss = 0.01653703\n",
      "Iteration 16, loss = 0.00860087\n",
      "Iteration 17, loss = 0.01265896\n",
      "Iteration 18, loss = 0.00535333\n",
      "Iteration 19, loss = 0.00692815\n",
      "Iteration 20, loss = 0.01181867\n",
      "Iteration 21, loss = 0.00791213\n",
      "Iteration 22, loss = 0.00428055\n",
      "Iteration 23, loss = 0.00395473\n",
      "Iteration 24, loss = 0.01056394\n",
      "Iteration 25, loss = 0.01230655\n",
      "Iteration 26, loss = 0.00623483\n",
      "Iteration 27, loss = 0.01067754\n",
      "Iteration 28, loss = 0.00811398\n",
      "Iteration 29, loss = 0.01218561\n",
      "Iteration 30, loss = 0.00778644\n",
      "Iteration 31, loss = 0.00294577\n",
      "Iteration 32, loss = 0.00172613\n",
      "Iteration 33, loss = 0.00064773\n",
      "Iteration 34, loss = 0.00057673\n",
      "Iteration 35, loss = 0.00056452\n",
      "Iteration 36, loss = 0.00055669\n",
      "Iteration 37, loss = 0.00055030\n",
      "Iteration 38, loss = 0.00054467\n",
      "Iteration 39, loss = 0.00053923\n",
      "Iteration 40, loss = 0.00053385\n",
      "Iteration 41, loss = 0.00052839\n",
      "Iteration 42, loss = 0.00052267\n",
      "Iteration 43, loss = 0.00051652\n",
      "Iteration 44, loss = 0.00050965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training completed.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1343\n",
      "           1       0.99      0.99      0.99      1600\n",
      "           2       0.97      0.97      0.97      1380\n",
      "           3       0.98      0.97      0.97      1433\n",
      "           4       0.97      0.98      0.97      1295\n",
      "           5       0.97      0.97      0.97      1273\n",
      "           6       0.98      0.99      0.98      1396\n",
      "           7       0.97      0.98      0.97      1503\n",
      "           8       0.96      0.96      0.96      1357\n",
      "           9       0.97      0.96      0.97      1420\n",
      "\n",
      "    accuracy                           0.98     14000\n",
      "   macro avg       0.98      0.98      0.98     14000\n",
      "weighted avg       0.98      0.98      0.98     14000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1323    1    2    0    0    2    6    4    5    0]\n",
      " [   1 1584    3    3    1    0    0    6    2    0]\n",
      " [   3    5 1341    5    5    2    1    7   10    1]\n",
      " [   0    1    8 1392    1   12    0    9    4    6]\n",
      " [   0    1    2    1 1266    1    4    3    3   14]\n",
      " [   1    1    0   11    4 1232   11    3   10    0]\n",
      " [   2    1    1    0    7    3 1378    0    4    0]\n",
      " [   3    2   11    1    5    2    0 1466    3   10]\n",
      " [   3    6    7    9    2    5    5    8 1304    8]\n",
      " [   5    2    2    5   16    5    0    8    9 1368]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(name=\"mnist_784\", version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(\"int\")\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model configuration\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=100,\n",
    "    batch_size=128,\n",
    "    alpha=0.0001,\n",
    "    random_state=42,\n",
    "    verbose=True  # Enable verbose to monitor convergence\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate the capability of the different learning approaches to find a good model, when we know that a very accurate model exists. For this, we add a 'cheat column' to our data: we add an additional column to the data matrix that simply contains a 0/1 encoding of the actual class label: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit0 = 3\n",
    "digit1 = 7\n",
    "\n",
    "cheatcol=np.array(mnist_bin_target) #making a copy of the original target array\n",
    "cheatcol[cheatcol==digit0]=0  #re-coding the two classes as 0s and 1s\n",
    "cheatcol[cheatcol==digit1]=1\n",
    "\n",
    "# The type of the target array is originally 'object' (the values '0','1',...,'9' are seen as categorical labels,\n",
    "# not as numbers). We now want to use the 0's and 1's as numbers: \n",
    "cheatcol=cheatcol.astype(float)\n",
    "\n",
    "cheatcol=np.reshape(cheatcol,[mnist_bin_data.shape[0],1]) #getting the dimensions right for the following .hstack operation to work ... \n",
    "mnist_bin_data_cheat = np.hstack((mnist_bin_data,cheatcol)) #appending the new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our candidate model types now are able, in principle, to construct a 100% accurate classifier for this data: we only have to 'learn' that only the last column in the data matters, and we can predict 'digit0' if we find a 0 in the last column and 'digit1' if we find a 1. All our SVM or NN network models would in principle be able to do just this, through a suitable setting of the SVM coefficients, respectively the NN weights.\n",
    "\n",
    "**Exercise 2:** \n",
    "\n",
    "**a** Describe, briefly, how the coefficients and weights of an SVM and NN model (with a suitably chosen number of layers) would have to be set, so that the resulting model is 100% accurate on this cheating data. Only consider the accuracy of the SVM or NN classifier defined by the coefficients/weights. You need not take into account that the SVM satisfies the max-margin objective, or that the NN minimizes its error function. This part of the exercise does not involve any Python code. Just give your answer in a short text.\n",
    "\n",
    "**b** Investigate how the accuracy of different SVM and NN classifiers improves in practice on this new dataset. Do you achieve 100% accuracy on the test set? If not, try to change the encoding in the cheat column: instead of representing digit1 with a 1, use a larger number, e.g. 250. Does that help? Why? This part of the exercise is in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 - Answers**\n",
    "\n",
    "**a**\n",
    "For the NN models the model should increase the weight of the cheat column, essentially making it more important while decreasing the weights of all other features. Ideally all other features weight should be as close to 0 as possible while the cheat feature should be as close to 1 as possible. This way it would always or almost always be able to get the correct prediction versus true value.\n",
    "\n",
    "For SVM the thinking is similar in that the coefficients for non-cheat features should be set to 0 or as close to as possible, while for the cheat feature the model can maximize the margin and achieve a separation what should result in 100% accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Classifier Train Accuracy (%) Test Accuracy (%)\n",
      "SVC (Original Encoding)             100.00            100.00\n",
      " NN (Original Encoding)             100.00            100.00\n",
      "       SVC (Re-encoded)             100.00            100.00\n",
      "        NN (Re-encoded)             100.00            100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KennethElong(KEEL)\\IdeaProjects\\firstPythonProject\\firstPyhtonProject\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the cheat column has already been prepared\n",
    "# mnist_bin_data_cheat is available as input, and mnist_bin_target is the binary target array\n",
    "\n",
    "# Split data into train and test sets with default split (25% test size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist_bin_data_cheat, mnist_bin_target, random_state=42)\n",
    "\n",
    "# SVC classifier\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_svc_train = svc.predict(X_train)\n",
    "y_pred_svc_test = svc.predict(X_test)\n",
    "svc_train_accuracy = accuracy_score(y_train, y_pred_svc_train) * 100\n",
    "svc_test_accuracy = accuracy_score(y_test, y_pred_svc_test) * 100\n",
    "\n",
    "# Neural Network classifier (Single-layer Perceptron)\n",
    "nn = MLPClassifier(hidden_layer_sizes=(), max_iter=1000, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred_nn_train = nn.predict(X_train)\n",
    "y_pred_nn_test = nn.predict(X_test)\n",
    "nn_train_accuracy = accuracy_score(y_train, y_pred_nn_train) * 100\n",
    "nn_test_accuracy = accuracy_score(y_test, y_pred_nn_test) * 100\n",
    "\n",
    "# Change cheat column encoding: use 250 for digit1\n",
    "cheatcol_encoded = np.where(mnist_bin_target == 1, 250, 0).astype(float)\n",
    "cheatcol_encoded = np.reshape(cheatcol_encoded, [mnist_bin_data.shape[0], 1])\n",
    "mnist_bin_data_cheat_encoded = np.hstack((mnist_bin_data, cheatcol_encoded))\n",
    "\n",
    "# Split the re-encoded data\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(mnist_bin_data_cheat_encoded, mnist_bin_target, random_state=42)\n",
    "\n",
    "# Train and test SVC on re-encoded data\n",
    "svc.fit(X_train_enc, y_train_enc)\n",
    "y_pred_svc_train_enc = svc.predict(X_train_enc)\n",
    "y_pred_svc_test_enc = svc.predict(X_test_enc)\n",
    "svc_train_accuracy_enc = accuracy_score(y_train_enc, y_pred_svc_train_enc) * 100\n",
    "svc_test_accuracy_enc = accuracy_score(y_test_enc, y_pred_svc_test_enc) * 100\n",
    "\n",
    "# Train and test NN on re-encoded data\n",
    "nn.fit(X_train_enc, y_train_enc)\n",
    "y_pred_nn_train_enc = nn.predict(X_train_enc)\n",
    "y_pred_nn_test_enc = nn.predict(X_test_enc)\n",
    "nn_train_accuracy_enc = accuracy_score(y_train_enc, y_pred_nn_train_enc) * 100\n",
    "nn_test_accuracy_enc = accuracy_score(y_test_enc, y_pred_nn_test_enc) * 100\n",
    "\n",
    "# Results\n",
    "results = pd.DataFrame({\n",
    "    \"Classifier\": [\"SVC (Original Encoding)\", \"NN (Original Encoding)\", \"SVC (Re-encoded)\", \"NN (Re-encoded)\"],\n",
    "    \"Train Accuracy (%)\": [\n",
    "        f\"{svc_train_accuracy:.2f}\", f\"{nn_train_accuracy:.2f}\",\n",
    "        f\"{svc_train_accuracy_enc:.2f}\", f\"{nn_train_accuracy_enc:.2f}\"\n",
    "    ],\n",
    "    \"Test Accuracy (%)\": [\n",
    "        f\"{svc_test_accuracy:.2f}\", f\"{nn_test_accuracy:.2f}\",\n",
    "        f\"{svc_test_accuracy_enc:.2f}\", f\"{nn_test_accuracy_enc:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the results table\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selected configuration of the SVC and the NN, both the SVC and the NN was able to perfectly predict the true values both in the scenarios of using 0 and 1 to encode the cheat column and when 1 is replaced by 250."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** (Now back to the data without a cheating column!) Suppose you want to design a custom kernel function for the MNIST data that better captures the relevant properties of 'similarity' in this data than the generic 'rbf' or 'poly' kernels. Bear in mind that the data as seen by our classifiers and kernel functions just consists of arrays of length 784. \n",
    "\n",
    "Describe one or two ideas for defining such a kernel. You need not show that the kernel you propose actually is positive semi-definite (though as a bonus, you can try to provide some arguments for that). This is a text-only exercise -- no sklearn code required!\n",
    "\n",
    "If you are really curious, you can implement your kernel as a function, and use it as a custom kernel. See http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py for an example of how that is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "**Answer:** one option is to use a convolutional NN model. These model are arranged in a 2D array and are better able to capture local patterns. This model is designed to learn spatial hierarchies of features and for the task with the predicting a number based on a handwritten input, such a model would be expectably fare well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
